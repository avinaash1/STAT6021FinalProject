---
title: "Data Analysis"
author: "Jordan Hiatt"
date: "12/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(corrplot)
library(GGally)
library(PerformanceAnalytics)
library(plotly)
```


```{r}
# Jordan

# save the data separately
jh_data <- read.csv(file = 'NBA_salary_stats.csv', row.names = 1, header = TRUE)

# Correlation plot (Khoi's code)
corrplot(cor(jh_data[, -c(1, 3, 4, 30, 38, 45)],
# corrplot(cor(jh_data[c(2,10)],
        use = 'complete.obs'),
        method = 'circle',
        type = 'upper')

# Just correlation of all vars with y
correlations<-cor(jh_data[, -c(1, 3, 4, 30, 38, 45)])[ ,1]

# save the highly correlated variables (above .5 is my threshold)
high_cor <- Filter(function(x) x>.5,correlations)

# remove the y variable
high_cor<-high_cor[2:length(high_cor)]

# Loop through all highly correlated vars to get regressions and plots
for (c in names(high_cor)) {
  # get the model for each highly correlated predictor
  l<-lm(jh_data$Actual~jh_data[[c]])
  
  # plot the model with the regression line
  plot(jh_data$Actual~jh_data[[c]], main=c)
  abline(l,col='red')
  
  # residual plots
  # these residual plots look really good. The only note is that the higher end errors are a little bigger making the center above the clear mass of residuals in most plots.
  plot(residuals(l), main = c)
  abline(h=0)
}

# see chap 8 and chap 3
```

```{r}
for (c in 1:ncol(jh_data)) {
  (is.numeric(jh_data[c]))
  }
```

```{r}
### Khoi

# exploratory data analysis
## reading in data
kt_data <- read.csv(file = 'NBA_salary_stats.csv', 
                    row.names = 1, 
                    header = TRUE)

# drop Guaranteed salary in exchange for using 'X2019.20' (actual pay) when predictingsalary (index = 3rd column)
### correlation plots, 2019-20 season performance stats
## correlation plots for total point scoring
corrplot(cor(subset(kt_data, select = c('Actual', 'MP', 'G', 'GS', 'X2P', 'X3P', 'FT', 
                                        'ORB', 'DRB', 'AST', 'STL', 'BLK', 'TOV', 'PF')),
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

## correlation plots with accuracy (e.g. 3-point percentage)
corrplot(cor(subset(kt_data, select = c('Actual', 'MP', 'G', 'GS.', 'X2P.', 'X3P.', 'FT.', 
                                        'ORB', 'DRB', 'AST', 'STL', 'BLK', 'TOV', 'PF')),
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

### correlation plots, overall career statistics
# e.g. mid-season trades, experience, age, etc.
corrplot(cor(subset(kt_data, select = c('Actual', 'yrs_exp', 'age_yrs', 'draft_num', 
                                        'num_accolades', 'NBA_titles', 'all_NBA', 'num_teams_1920', 
                                        'mid_season_trades', 'num_yrs_current_team', 
                                        'num_teams_played', 'injury_yrs', 'twitter_followers')), 
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

## list variables most correlated to actual salary
cor(subset(kt_data, select = c('Actual', 'MP', 'G', 'GS', 'X2P', 'X3P', 'FT', 
                               'ORB', 'DRB', 'AST', 'STL', 'BLK', 'TOV', 'PF')))[,1]
# GS, X2P, FT, DRB, AST, TOV are > 0.5
# MP, X3P, ORB, STL, BLK, PF > 0.3
cor(subset(kt_data, select = c('Actual', 'MP', 'G', 'GS.', 'X2P.', 'X3P.', 'FT.', 
                               'ORB', 'DRB', 'AST', 'STL', 'BLK', 'TOV', 'PF')))[,1]
# GS., DRB, AST, TOV are > 0.5
# MP, ORB, STL, BLK, PF > 0.3
cor(subset(kt_data, select = c('Actual', 'yrs_exp', 'age_yrs', 'draft_num', 
                               'num_accolades', 'NBA_titles', 'all_NBA', 'num_teams_1920', 
                               'mid_season_trades', 'num_yrs_current_team', 
                               'num_teams_played', 'injury_yrs', 'twitter_followers')))[,1]
# yrs_exp, num_accolades are 0.5
# age_yrs, all_NBA, num_yrs_current_team, twitter_followers > 0.3
```

```{r}
# Jordan
# automated search procedures

# !! Do we want to split the data and do crosswise regression?
##intercept only model
regnull <- lm(jh_data$Actual~1, data=jh_data)

##model with all predictors
regfull <- lm(Actual~.-Player-Guaranteed-url, data=jh_data)

# step_reg <- step(regnull, scope=list(lower=regnull, upper=regfull), direction="both")
```

```{r}
# model evaluation see chapters 5 and 7
library(faraway)
jh_result<-lm(Actual ~ num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB + GS + eFG. + 
    all_NBA, data = jh_data)

summary(jh_result)
anova(jh_result)

vif(jh_result)
# multicolinearity appears to be an issue with GS and potentially eFG., num_teams_1920, and GS.
#correlation between variables?

vcov(jh_result)[,c('GS','eFG.',"num_teams_1920","GS.")]
```
From the VCOV table, looking at rows specifically:
GS - 
```{r}
# let's try dropping GS (highest VIF) and eFG. (also a major one) and see how things change
jh_result.2<-lm(Actual ~ num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB + 
    all_NBA, data = jh_data)
summary(jh_result.2)
# Adj R2 is about the same
anova(jh_result.2)
vif(jh_result.2)
# much better VIF
```

```{r}
#partial f test, if this has p>0.05 we reject null and keep full model.
# remove all_NBA
reduced.1 <- lm(Actual~num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB, data = jh_data)
anova(reduced.1,jh_result.2) 
summary(reduced.1)

# we fail to reject the null so let's go with the smaller model

```


```{r}
library(MASS)
# Do we need to transform predictor? If lambda=1 is in the confidence interval we do not need a transformation, if 0 is in CI then us ln(y), otherwise use y^lambda (see pg 17 in notes)
boxcox(jh_result)
# check distribution of residuals
plot(residuals(jh_result), main = c)
abline(h=0,col='red')
# check normal distribution
qqnorm(jh_result$residuals)
qqline(jh_result$residuals, col="red")
# autocorrelation
acf(jh_result$residuals, main="ACF of Residuals")
```

Questions:
1. Is it a problem if we don't have categorical variables?
2. VIF suggests we have some multicolinearity, do we drop those with high VIF's?
3. Do we want to error on the side of more predictors (less bias) or fewer predictors (less variance)?
4. Should we do cross validation?
5. Should we check AIC/BIC/Press statistic (43)
6. Test predictive ability?
7. Should we try to get better linear fits by dealing with outliers?
8. (partial regression plot, pg. 48, module 8)


```{r}
jh_data$Pos<-factor(jh_data$Pos)
is.factor(jh_data$Pos)

levels(jh_data$Pos)
contrasts(jh_data$Pos)

a1<-subset(jh_data,Pos=="C")
a2<-subset(jh_data,Pos=="C-PF")
a3<-subset(jh_data,Pos=="PF")
a4<-subset(jh_data,Pos=="PF-C")
a5<-subset(jh_data,Pos=="PF-SF")
a6<-subset(jh_data,Pos=="PG")
a7<-subset(jh_data,Pos=="PG-SG")
a8<-subset(jh_data,Pos=="SF")
a9<-subset(jh_data,Pos=="SF-C")
a10<-subset(jh_data,Pos=="SF-PF")
a11<-subset(jh_data,Pos=="SF-SG")
a12<-subset(jh_data,Pos=="SG")
a13<-subset(jh_data,Pos=="SG-PG")
a14<-subset(jh_data,Pos=="SG-SF")


reg1<-lm(Guaranteed~PTS,data=a1)
reg2<-lm(Guaranteed~PTS,data=a2)
reg3<-lm(Guaranteed~PTS,data=a3)
reg4<-lm(Guaranteed~PTS,data=a4)
reg5<-lm(Guaranteed~PTS,data=a5)
reg6<-lm(Guaranteed~PTS,data=a6)
reg7<-lm(Guaranteed~PTS,data=a7)
reg8<-lm(Guaranteed~PTS,data=a8)
reg9<-lm(Guaranteed~PTS,data=a9)
reg10<-lm(Guaranteed~PTS,data=a10)
reg11<-lm(Guaranteed~PTS,data=a11)
reg12<-lm(Guaranteed~PTS,data=a12)
reg13<-lm(Guaranteed~PTS,data=a13)
reg14<-lm(Guaranteed~PTS,data=a14)

plot(jh_data$PTS,jh_data$Guaranteed, main="Guaranteed against PTS, by Pos")
points(a2$PTS,a2$Guaranteed, pch=2, col="red")
points(a3$PTS,a3$Guaranteed, pch=3, col="blue")
points(a4$PTS,a4$Guaranteed, pch=4, col="green")
points(a5$PTS,a5$Guaranteed, pch=5, col="orange")
points(a6$PTS,a6$Guaranteed, pch=6, col="purple")
points(a7$PTS,a7$Guaranteed, pch=7, col="pink")
points(a8$PTS,a8$Guaranteed, pch=8, col="seagreen2")
points(a9$PTS,a9$Guaranteed, pch=9, col="salmon3")
points(a10$PTS,a10$Guaranteed, pch=10, col="royalblue4")
points(a11$PTS,a11$Guaranteed, pch=11, col="rosybrown4")
points(a12$PTS,a12$Guaranteed, pch=12, col="skyblue2")
points(a13$PTS,a13$Guaranteed, pch=13, col="slateblue2")
points(a14$PTS,a14$Guaranteed, pch=14, col="slategray2")

abline(reg1,lty=1)
# abline(reg2,lty=2, col="red")
abline(reg3,lty=3, col="blue")
abline(reg4,lty=4, col="green")
abline(reg5,lty=5, col="orange")
abline(reg6,lty=6, col="purple")
# abline(reg7,lty=7, col="pink")
abline(reg8,lty=8, col="seagreen2")
# abline(reg9,lty=9, col="salmon3")
abline(reg10,lty=10, col="royalblue4")
# abline(reg11,lty=11, col="rosybrown4")
abline(reg12,lty=12, col="skyblue2")
abline(reg13,lty=13, col="slateblue2")
# abline(reg14,lty=14, col="slategray2")
```

