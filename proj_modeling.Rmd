---
title: "Data Analysis"
author: "Jordan Hiatt"
date: "12/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(corrplot)
library(GGally)
library(PerformanceAnalytics)
library(plotly)
```


```{r}
# Jordan

# save the data separately
jh_data <- read.csv(file = 'NBA_salary_stats.csv', row.names = 1, header = TRUE)

# Correlation plot (Khoi's code)
corrplot(cor(jh_data[, -c(1, 3, 4, 30, 38, 45)],
# corrplot(cor(jh_data[c(2,10)],
        use = 'complete.obs'),
        method = 'circle',
        type = 'upper')

# Just correlation of all vars with y
correlations<-cor(jh_data[, -c(1, 3, 4, 30, 38, 45)])[ ,1]

# save the highly correlated variables (above .5 is my threshold)
high_cor <- Filter(function(x) x>.5,correlations)

# remove the y variable
high_cor<-high_cor[2:length(high_cor)]

# Loop through all highly correlated vars to get regressions and plots
for (c in names(high_cor)) {
  # get the model for each highly correlated predictor
  l<-lm(jh_data$Actual~jh_data[[c]])
  
  # plot the model with the regression line
  plot(jh_data$Actual~jh_data[[c]], main=c)
  abline(l,col='red')
  
  # residual plots
  # these residual plots look really good. The only note is that the higher end errors are a little bigger making the center above the clear mass of residuals in most plots.
  plot(residuals(l), main = c)
  abline(h=0)
}

# see chap 8 and chap 3
```

```{r}
for (c in 1:ncol(jh_data)) {
  (is.numeric(jh_data[c]))
  }
```

```{r}
# exploratory data analysis
## reading in data
kt_data <- read.csv(file = 'NBA_salary_stats.csv', 
                    row.names = 1, 
                    header = TRUE)
```

```{r}
# drop Guaranteed salary in exchange for using Actual pay when predicting salary (index = 3rd column)
### correlation plots, 2019-20 season performance stats
## correlation plots for total point scoring
other_perf <- c('ORB', 'DRB', 'AST', 'STL', 'BLK', 'TOV', 'PF')
performance_stats <- c('Actual', 'MP', 'GS', 'X2P', 'X3P', 'FT')
corrplot(cor(subset(kt_data, select = c(performance_stats, other_perf)),
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = performance_stats)) %>% 
  pairs()
```

```{r}
## correlation plots with accuracy (e.g. 3-point percentage)
performance_stats_pct <- c('Actual', 'MP', 'GS.', 'X2P.', 'X3P.', 'FT.')
corrplot(cor(subset(kt_data, select = c(performance_stats_pct, other_perf)),
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = performance_stats_pct)) %>% 
  pairs()
```

```{r}
## correlation plots with attempts (e.g. 3-point attempts)
performance_stats_atmpt <- c('Actual', 'MP', 'G', 'X2PA', 'X3PA', 'FTA')
corrplot(cor(subset(kt_data, select = c(performance_stats_atmpt, other_perf)),
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = performance_stats_atmpt)) %>% 
  pairs()
```

```{r}
### correlation plots, overall career statistics
# e.g. mid-season trades, experience, age, etc.
other_stats <- c('Actual', 'yrs_exp', 'age_yrs', 'draft_num', 
                 'num_accolades', 'NBA_titles', 'all_NBA', 'num_teams_1920', 
                 'mid_season_trades', 'num_yrs_current_team', 
                 'num_teams_played', 'injury_yrs', 'twitter_followers')
corrplot(cor(subset(kt_data, select = other_stats), 
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = other_stats)) %>% 
  pairs()
```

```{r}
## list variables most correlated to actual salary
cor(subset(kt_data, select = performance_stats))[,1]
# GS, X2P, FT are > 0.5
# X3P > 0.38
# MP is a decent correlator in all performance-related correlations
cor(subset(kt_data, select = performance_stats_pct))[,1]
# GS., X2P. are > 0.5
# MP still a decent correlator
cor(subset(kt_data, select = performance_stats_atmpt))[,1]
# FTA and X2PA are > 0.5
# X3PA > 0.39
# MP still a decent correlator
cor(subset(kt_data, select = other_stats))[,1]
# yrs_exp, num_accolades are > 0.5
# age_yrs, all_NBA, num_yrs_current_team, twitter_followers > 0.3
cor(subset(kt_data, select = c('Actual', other_perf)))[,1]
# DRB, AST, TOV are > 0.5
# ORB, STL, BLK, and PF are > 0.3
```

```{r}
## regression with all variables but player name, guaranteed salary, link to url, and team
lm(Actual ~ ., data = kt_data[-c(1, 3, 30, 38)]) %>% 
  summary()
## regression with performance stats
lm(Actual ~ ., data = subset(kt_data, select = performance_stats)) %>% 
    summary()
## regression with performance stats, percentages
lm(Actual ~ ., data = subset(kt_data, select = performance_stats_pct)) %>% 
    summary()
## regression with performance stats, attempts
lm(Actual ~ ., data = subset(kt_data, select = performance_stats_atmpt)) %>% 
    summary()
## regression with other performance stats
lm(Actual ~ ., data = subset(kt_data, select = c('Actual', other_perf))) %>% 
    summary()
## regression with other stats
lm(Actual ~ ., data = subset(kt_data, select = other_stats)) %>% 
    summary()
```

```{r}
## all stats above
lm(Actual ~ ., data = subset(kt_data, select = c(performance_stats, performance_stats_pct, 
                                                 performance_stats_atmpt, other_perf, other_stats) %>% 
                                                  unique())) %>% 
    summary()

## regression with variables observed in correlation plots and previous regressions to be significant
lm(Actual ~ GS. + FT + yrs_exp + DRB + AST + TOV + 
            yrs_exp + num_accolades + num_teams_played, 
    data = kt_data) %>% 
      summary()

## further reduced regression
lm(Actual ~ GS. + FT + yrs_exp + yrs_exp + 
            num_accolades + num_teams_played, 
    data = kt_data) %>% 
      summary()
```

```{r}
## reduced dataset for further analysis
kt_data1 <- sapply(subset(kt_data, select = c(performance_stats, other_perf, 
                                              other_stats) %>% unique()), as.numeric) %>% 
              data.frame()
kt_data2 <- sapply(subset(kt_data, select = c(performance_stats, performance_stats_pct, 
                                              performance_stats_atmpt, other_perf, other_stats) %>% 
                                                unique()), as.numeric) %>% 
              data.frame()

## PCA
# first PCA, more reduced dataset
kt_pca1 <- prcomp(kt_data1, 
                  scale = TRUE)

# 95% of variance explained by first 14 PCAs
## after 14 PCAs, each successive PCA only explains <1% of additional variance
# 90% of variance explained by first 10 PCAs
## after 10 PCAs, each successive PCA only explains <2% of additional variance
# 75% of variance explained by first 6 PCAs
## after 6 PCAs, each successive PCA only explains <4% of additional variance

cumsum(kt_pca1$sdev^2 / sum(kt_pca1$sdev^2))

kt_pca1$rotation[, 1:14] %>% round(3) %>% abs()

# second PCA, less reduced dataset
kt_pca2 <- prcomp(kt_data2, 
                  scale = TRUE)

cumsum(kt_pca2$sdev^2 / sum(kt_pca2$sdev^2))

kt_pca2$rotation[, 1:14] %>% round(3) %>% abs()
```

```{r}
## lasso and  ridge regression
set.seed(19462)
nba_x <- model.matrix(Actual ~ ., kt_data2)[, -1]
nba_y <- kt_data2$Actual
# select a random sample
train <- sample(1:nrow(nba_x), nrow(nba_x) / 2)
# split data into testing
test_x <- (-train)
test_y <- nba_y[test_x]

# using CV to find the optimal lambda based on training set
set.seed(72347)

# fitting ridge regression
nba_cv <- cv.glmnet(nba_x[train, ],
                    nba_y[train],
                    alpha = 0)

NBA_ridge <- glmnet(nba_x[train,],
                    nba_y[train],
                    alpha = 0,
                    lambda = nba_cv$lambda.min,
                    thresh = 1e-14)
# test MSE with optimal lambda
NBA_pred <- predict(NBA_ridge,
                    s = nba_cv$lambda.min,
                    newx = nba_x[test_x,])
mean((NBA_pred - test_y)^2)

# fitting lasso regression
nba_cv1 <- cv.glmnet(nba_x[train, ],
                     nba_y[train],
                     alpha = 1)

NBA_lasso <- glmnet(nba_x[train,],
                    nba_y[train],
                    alpha = 1,
                    lambda = nba_cv1$lambda.min,
                    thresh = 1e-14)
# test MSE with optimal lambda
NBA_pred1 <- predict(NBA_lasso,
                     s = nba_cv$lambda.min,
                     newx = nba_x[test_x,])
mean((NBA_pred1 - test_y)^2)

# fitting OLS
NBA_OLS <- glmnet(nba_x[train,],
                  nba_y[train],
                  alpha = 0,
                  lambda = 0,
                  thresh = 1e-14)

# test MSE with optimal lambda
NBA_pred2 <- predict(NBA_OLS,
                     s = 0,
                     newx = nba_x[test_x,])
mean((NBA_pred2 - test_y)^2)

# test MSE with general linear regressions
mean((predict(lm(Actual ~ ., data = kt_data2), 
              newx = nba_x[test_x,]) - test_y)^2)
mean((predict(lm(Actual ~ GS. + FT + yrs_exp + yrs_exp + 
                          num_accolades + num_teams_played, 
                 data = kt_data2), 
              newx = nba_x[test_x,]) - test_y)^2)

## use predict() with some of the previous regressions to compare MSEs

cbind(coefficients(NBA_ridge),
      coefficients(NBA_lasso),
      coefficients(NBA_OLS)) %>% 
  round(3)
```

```{r}
# Jordan
# automated search procedures

# !! Do we want to split the data and do crosswise regression?
##intercept only model
regnull <- lm(jh_data$Actual~1, data=jh_data)

##model with all predictors
regfull <- lm(Actual~.-Player-Guaranteed-url, data=jh_data)

# step_reg <- step(regnull, scope=list(lower=regnull, upper=regfull), direction="both")
```

```{r}
# model evaluation see chapters 5 and 7
library(faraway)
jh_result<-lm(Actual ~ num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB + GS + eFG. + 
    all_NBA, data = jh_data)

summary(jh_result)
anova(jh_result)

vif(jh_result)
# multicolinearity appears to be an issue with GS and potentially eFG., num_teams_1920, and GS.
#correlation between variables?

vcov(jh_result)[,c('GS','eFG.',"num_teams_1920","GS.")]
```
From the VCOV table, looking at rows specifically:
GS - 
```{r}
# let's try dropping GS (highest VIF) and eFG. (also a major one) and see how things change
jh_result.2<-lm(Actual ~ num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB + 
    all_NBA, data = jh_data)
summary(jh_result.2)
# Adj R2 is about the same
anova(jh_result.2)
vif(jh_result.2)
# much better VIF
```

```{r}
#partial f test, if this has p>0.05 we reject null and keep full model.
# remove all_NBA
reduced.1 <- lm(Actual~num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB, data = jh_data)
anova(reduced.1,jh_result.2) 
summary(reduced.1)

# we fail to reject the null so let's go with the smaller model

```


```{r}
library(MASS)
# Do we need to transform predictor? If lambda=1 is in the confidence interval we do not need a transformation, if 0 is in CI then us ln(y), otherwise use y^lambda (see pg 17 in notes)
boxcox(reduced.1)
# check distribution of residuals
plot(residuals(reduced.1))
abline(h=0,col='red')
# check normal distribution
qqnorm(reduced.1$residuals)
qqline(reduced.1$residuals, col="red")
# autocorrelation
acf(reduced.1$residuals, main="ACF of Residuals")


#--------------------
# Transformation
jh_data$Actual.t <- jh_data$Actual^(1/4)
reduced.t<- lm(Actual.t~num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB, data = jh_data)
summary(reduced.t)
anova(reduced.t)

#check assumptions on transformed data:
boxcox(reduced.t)
# check distribution of residuals
plot(residuals(reduced.t))
abline(h=0,col='red')
# check normal distribution
qqnorm(reduced.t$residuals)
qqline(reduced.t$residuals, col="red")
# autocorrelation
acf(reduced.t$residuals, main="ACF of Residuals")
# lag of 24ish is above ci, do we need to worry about this?

```
```{r}
stepBoth <- lm(jh_data$Actual.t ~ yrs_exp + num_teams_played + GS. + 
    drafted  + G + Pos + AST + draft_num + DRB + twitter_followers + 
    FGA + PF + injury_yrs, data=jh_data)
stepBoth
summary(stepBoth)
```

Questions:
1. Is it a problem if we don't have categorical variables? (no but we wil anyway)
2. Is interaction only for categorical variables (Answer: No, but should we worry about it?  Are there likely candidates? Should we simplify?)
(a) How do we find a good interaction with just 1 categorical variable?
3. Do we want to error on the side of more predictors (less bias) or fewer predictors (less variance)?
4. Should we do cross validation?
5. Should we check AIC/BIC/Press statistic (43)
6. Test predictive ability?
7. Should we try to get better linear fits by dealing with outliers? (apparenty we don't have to check categorical predictors for outliers (see discussion notes))
8. (partial regression plot, pg. 48, module 8)


```{r}
jh_data$Pos<-factor(jh_data$Pos)
is.factor(jh_data$Pos)

levels(jh_data$Pos)
contrasts(jh_data$Pos)

# equality of variance assumption test.  H0 variances are the same, HA variances are not the same for all classes
library(lawstat)
levene.test(jh_data$Actual.t,jh_data$Pos)
# greater than 0.05 so we reject H0 and we are in good shape

a1<-subset(jh_data,Pos=="C")
a2<-subset(jh_data,Pos=="C-PF")
a3<-subset(jh_data,Pos=="PF")
a4<-subset(jh_data,Pos=="PF-C")
a5<-subset(jh_data,Pos=="PF-SF")
a6<-subset(jh_data,Pos=="PG")
a7<-subset(jh_data,Pos=="PG-SG")
a8<-subset(jh_data,Pos=="SF")
a9<-subset(jh_data,Pos=="SF-C")
a10<-subset(jh_data,Pos=="SF-PF")
a11<-subset(jh_data,Pos=="SF-SG")
a12<-subset(jh_data,Pos=="SG")
a13<-subset(jh_data,Pos=="SG-PG")
a14<-subset(jh_data,Pos=="SG-SF")


reg1<-lm(Actual.t~PTS,data=a1)
reg2<-lm(Actual.t~PTS,data=a2)
reg3<-lm(Actual.t~PTS,data=a3)
reg4<-lm(Actual.t~PTS,data=a4)
reg5<-lm(Actual.t~PTS,data=a5)
reg6<-lm(Actual.t~PTS,data=a6)
reg7<-lm(Actual.t~PTS,data=a7)
reg8<-lm(Actual.t~PTS,data=a8)
reg9<-lm(Actual.t~PTS,data=a9)
reg10<-lm(Actual.t~PTS,data=a10)
reg11<-lm(Actual.t~PTS,data=a11)
reg12<-lm(Actual.t~PTS,data=a12)
reg13<-lm(Actual.t~PTS,data=a13)
reg14<-lm(Actual.t~PTS,data=a14)

plot(jh_data$PTS,jh_data$Actual.t, main="Actual income against PTS, by Pos")
points(a2$PTS,a2$Actual.t, pch=2, col="red")
points(a3$PTS,a3$Actual.t, pch=3, col="blue")
points(a4$PTS,a4$Actual.t, pch=4, col="green")
points(a5$PTS,a5$Actual.t, pch=5, col="orange")
points(a6$PTS,a6$Actual.t, pch=6, col="purple")
points(a7$PTS,a7$Actual.t, pch=7, col="pink")
points(a8$PTS,a8$Actual.t, pch=8, col="seagreen2")
points(a9$PTS,a9$Actual.t, pch=9, col="salmon3")
points(a10$PTS,a10$Actual.t, pch=10, col="royalblue4")
points(a11$PTS,a11$Actual.t, pch=11, col="rosybrown4")
points(a12$PTS,a12$Actual.t, pch=12, col="skyblue2")
points(a13$PTS,a13$Actual.t, pch=13, col="slateblue2")
points(a14$PTS,a14$Actual.t, pch=14, col="slategray2")

abline(reg1,lty=1)
# abline(reg2,lty=2, col="red")
abline(reg3,lty=3, col="blue")
abline(reg4,lty=4, col="green")
abline(reg5,lty=5, col="orange")
abline(reg6,lty=6, col="purple")
# abline(reg7,lty=7, col="pink")
abline(reg8,lty=8, col="seagreen2")
# abline(reg9,lty=9, col="salmon3")
abline(reg10,lty=10, col="royalblue4")
# abline(reg11,lty=11, col="rosybrown4")
abline(reg12,lty=12, col="skyblue2")
abline(reg13,lty=13, col="slateblue2")
# abline(reg14,lty=14, col="slategray2")

legend("right", c("C","C-PF","PF","PF-C","PF-SF","PG","PG-SG","SF","SF-C","SF-PF","SF-SG","SG","SG-PG","SG-SF"), lty=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14), pch=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14), col=c("black","red","blue","green","orange","purple","pink","seagreen2","salmon3","royalblue4","rosybrown4","skyblue2","slateblue2","slategray2"))

# some of these lines are a little nuts (PF-SF, SG-PG), these should be collapsed into other fields

# run a boxplot to see if it makes sense to combine others of these
boxplot(jh_data$Actual.t~jh_data$Pos, main="Boxplot of Actual Pay by Position")

# colapse classes
# create new collapsed class
jh_data$Pos.colapsed <- jh_data$Pos

# Eliminate all the double position classes. When no clear preference in the box plot pick the primary position.

# C-PF doesn't really fit anywhere, put with C
jh_data$Pos.colapsed[which(jh_data$Pos == "C-PF")] = "C"
# PF-C closer to C than PF. 
jh_data$Pos.colapsed[which(jh_data$Pos == "PF-C")] = "C"
# PF-SF is unique, we will keep that one around
# PG-SG we can group with PG (no natural match)
jh_data$Pos.colapsed[which(jh_data$Pos == "PG-SG")] = "PG"
# SF-C we will group with SF, no natural match
jh_data$Pos.colapsed[which(jh_data$Pos == "SF-C")] = "SF"
# SF-PF seems like a more natural match with SF
jh_data$Pos.colapsed[which(jh_data$Pos == "SF-PF")] = "SF"
# SF-SG seems like a better match with SF
jh_data$Pos.colapsed[which(jh_data$Pos == "SF-SG")] = "SF"
# SG-PG we will go with SG, no obvious match
jh_data$Pos.colapsed[which(jh_data$Pos == "SG-PG")] = "SF"
# SG-SF we will go with SG, no obvious match
jh_data$Pos.colapsed[which(jh_data$Pos == "SG-SF")] = "SF"

boxplot(jh_data$Actual.t~jh_data$Pos.colapsed, main="Boxplot of Actual Pay by Colapsed Position")

# Check the new model
reduced.t2<-lm(jh_data$Actual.t ~ yrs_exp + num_teams_played + GS. +
    drafted  + G + Pos.colapsed + AST + draft_num + DRB + twitter_followers +
    FGA + PF + injury_yrs, data=jh_data)
summary(reduced.t2)

# equality of variance assumption test.  H0 variances are the same, HA variances are not the same for all classes
library(lawstat)
levene.test(jh_data$Actual.t,jh_data$Pos.colapsed)
# greater than 0.05 so we reject H0 and we are in good shape

# replot the graph, look for nonlinear relationships (not paralel lines) add in interaction and compare partial f tests
b1<-subset(jh_data,Pos.colapsed=="C")
b2<-subset(jh_data,Pos.colapsed=="PF")
b3<-subset(jh_data,Pos.colapsed=="PF-SF")
b4<-subset(jh_data,Pos.colapsed=="PG")
b5<-subset(jh_data,Pos.colapsed=="SF")
b6<-subset(jh_data,Pos.colapsed=="SG")


reg1<-lm(Actual.t~PTS,data=b1)
reg2<-lm(Actual.t~PTS,data=b2)
reg3<-lm(Actual.t~PTS,data=b3)
reg4<-lm(Actual.t~PTS,data=b4)
reg5<-lm(Actual.t~PTS,data=b5)
reg6<-lm(Actual.t~PTS,data=b6)

plot(jh_data$PTS,jh_data$Actual.t, main="Actual income against PTS, by Pos")
points(b2$PTS,b2$Actual.t, pch=2, col="red")
points(b3$PTS,b3$Actual.t, pch=3, col="blue")
points(b4$PTS,b4$Actual.t, pch=4, col="green")
points(b5$PTS,b5$Actual.t, pch=5, col="orange")
points(b6$PTS,b6$Actual.t, pch=6, col="purple")

abline(reg1,lty=1)
abline(reg2,lty=2, col="red")
abline(reg3,lty=3, col="blue")
abline(reg4,lty=4, col="green")
abline(reg5,lty=5, col="orange")
abline(reg6,lty=6, col="purple")

legend("right", c("C","PF","PF-SF","PG","SF","SG"), lty=c(1,2,3,4,5,6), pch=c(1,2,3,4,5,6), col=c("black","red","blue","green","orange","purple"))


# lines are not paralel indicating a posible interaction.  Let's see what that model looks like.
reduced.t3<-lm(jh_data$Actual.t ~ yrs_exp + num_teams_played + GS. +
    drafted  + G * Pos.colapsed + AST + draft_num + DRB + twitter_followers +
    FGA + PF + injury_yrs, data=jh_data)
summary(reduced.t3)

# partial f test, should we keep the interaction?
anova(reduced.t3,reduced.t2)
# fail to reject the null, we will remove the interaction
```

