---
title: "Data Analysis"
author: "Jordan Hiatt"
date: "12/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(corrplot)
library(GGally)
library(PerformanceAnalytics)
library(plotly)
```


```{r}
# Jordan

# save the data separately
jh_data <- read.csv(file = 'NBA_salary_stats.csv', row.names = 1, header = TRUE)

# Correlation plot (Khoi's code)
corrplot(cor(jh_data[, -c(1, 3, 4, 30, 38, 45)],
# corrplot(cor(jh_data[c(2,10)],
        use = 'complete.obs'),
        method = 'circle',
        type = 'upper')

# Just correlation of all vars with y
correlations<-cor(jh_data[, -c(1, 3, 4, 30, 38, 45)])[ ,1]

# save the highly correlated variables (above .5 is my threshold)
high_cor <- Filter(function(x) x>.5,correlations)

# remove the y variable
high_cor<-high_cor[2:length(high_cor)]

# Loop through all highly correlated vars to get regressions and plots
for (c in names(high_cor)) {
  # get the model for each highly correlated predictor
  l<-lm(jh_data$Actual~jh_data[[c]])
  
  # plot the model with the regression line
  plot(jh_data$Actual~jh_data[[c]], main=c)
  abline(l,col='red')
  
  # residual plots
  # these residual plots look really good. The only note is that the higher end errors are a little bigger making the center above the clear mass of residuals in most plots.
  plot(residuals(l), main = c)
  abline(h=0)
}

# see chap 8 and chap 3
```

```{r}
# exploratory data analysis
## reading in data
kt_data <- read.csv(file = 'NBA_salary_stats.csv', 
                    row.names = 1, 
                    header = TRUE)
```

```{r}
# drop Guaranteed salary in exchange for using Actual pay when predicting salary (index = 3rd column)
### correlation plots, 2019-20 season performance stats
## correlation plots for total point scoring
other_perf <- c('ORB', 'DRB', 'AST', 'STL', 'BLK', 'TOV', 'PF')
performance_stats <- c('Actual', 'MP', 'GS', 'X2P', 'X3P', 'FT')
corrplot(cor(subset(kt_data, select = c(performance_stats, other_perf)),
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = performance_stats)) %>% 
  pairs()
```

```{r}
## correlation plots with accuracy (e.g. 3-point percentage)
performance_stats_pct <- c('Actual', 'MP', 'GS.', 'X2P.', 'X3P.', 'FT.')
corrplot(cor(subset(kt_data, select = c(performance_stats_pct, other_perf)),
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = performance_stats_pct)) %>% 
  pairs()
```

```{r}
## correlation plots with attempts (e.g. 3-point attempts)
performance_stats_atmpt <- c('Actual', 'MP', 'G', 'X2PA', 'X3PA', 'FTA')
corrplot(cor(subset(kt_data, select = c(performance_stats_atmpt, other_perf)),
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = performance_stats_atmpt)) %>% 
  pairs()
```

```{r}
### correlation plots, overall career statistics
# e.g. mid-season trades, experience, age, etc.
other_stats <- c('Actual', 'yrs_exp', 'age_yrs', 'draft_num', 
                 'num_accolades', 'NBA_titles', 'all_NBA', 'num_teams_1920', 
                 'mid_season_trades', 'num_yrs_current_team', 
                 'num_teams_played', 'injury_yrs', 'twitter_followers', 'drafted')
corrplot(cor(subset(kt_data, select = other_stats), 
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = other_stats)) %>% 
  pairs()
```

```{r}
## list variables most correlated to actual salary
cor(subset(kt_data, select = performance_stats))[, 1]
# GS, X2P, FT are > 0.5
# X3P > 0.38
# MP is a decent correlator in all performance-related correlations
cor(subset(kt_data, select = performance_stats_pct))[, 1]
# GS. is > 0.5
cor(subset(kt_data, select = performance_stats_atmpt))[, 1]
# FTA and X2PA are > 0.5
# X3PA > 0.39
cor(subset(kt_data, select = other_stats))[, 1]
# yrs_exp, num_accolades are > 0.5
# age_yrs, all_NBA, num_yrs_current_team, twitter_followers > 0.3
cor(subset(kt_data, select = c('Actual', other_perf)))[, 1]
# DRB, AST, TOV are > 0.5
# ORB, STL, BLK, and PF are > 0.3
```

```{r}
## regression with all variables but player name, guaranteed salary, link to url, and team
lm(Actual ~ ., data = kt_data[-c(1, 3, 30, 38)]) %>% 
  summary()
## regression with performance stats
lm(Actual ~ ., data = subset(kt_data, select = performance_stats)) %>% 
    summary()
## regression with performance stats, percentages
lm(Actual ~ ., data = subset(kt_data, select = performance_stats_pct)) %>% 
    summary()
## regression with performance stats, attempts
lm(Actual ~ ., data = subset(kt_data, select = performance_stats_atmpt)) %>% 
    summary()
## regression with other performance stats
lm(Actual ~ ., data = subset(kt_data, select = c('Actual', other_perf))) %>% 
    summary()
## regression with other stats
lm(Actual ~ ., data = subset(kt_data, select = other_stats)) %>% 
    summary()
```

# Most Significant Predictor Variables:

- **G:** Number of games played in the 2019-2020 NBA season.
- **GS.:** Percentage of games started in the 2019-2020 NBA season.
- **GS.:** Percentage of games started in the 2019-2020 NBA season.
- **FT:** Free throws scored.
- **FTA:** Free throws attempted.
- **DRB:** Defensive rebounds.
- **AST:** Number of assists.
- **PF:** Number of personal fouls.
- **yrs_exp:** The number of years the player has played in the NBA.
- **num_accolades:** The number of notable accolades a player has.
- **NBA_titles:** The number of the NBA titles the player has won.
- **num_teams_1920:** The number of teams a player has played for during the 2019-2020 season.
- **num_teams_played:** The number of teams the player has played for throughout the duration of their career.

```{r}
## all stats from lists
lm(Actual ~ ., data = subset(kt_data, select = c(performance_stats, performance_stats_pct, 
                                                 performance_stats_atmpt, other_perf, other_stats) %>% 
                                                  unique())) %>% 
    summary()

## regression with variables observed in previous regressions to be significant
# use GS. alone instead of G and GS
lm(Actual ~ GS. + FT + FTA + DRB + AST + PF + 
            yrs_exp + num_accolades + NBA_titles + 
            num_teams_1920 + num_teams_played, 
    data = kt_data) %>% 
      summary()

## further reduced regression, more significant
kt_lm1 <- lm(Actual ~ GS. + DRB + PF + yrs_exp + 
                      num_accolades + NBA_titles + 
                      num_teams_1920 + num_teams_played, 
   data = kt_data) %>% 
    summary()

## stepwise regression
# list c(1, 3, 30, 38) excludes non-numeric variables (name, url, team) and guaranteed salary
step(lm(formula = Actual ~ 1, data = kt_data[-c(1, 3, 30, 38)]), 
     scope = list(lower = lm(formula = Actual ~ 1, data = kt_data[-c(1, 3, 30, 38)]), 
                  upper = lm(formula = Actual ~ ., data = kt_data[-c(1, 3, 30, 38)])), 
     direction = 'both')

## suggested regression
kt_lm2 <- lm(formula = Actual ~ num_accolades + PTS + yrs_exp + num_teams_played + 
                                GS. + NBA_titles + num_teams_1920 + PF + DRB + GS + 
                                X2P. + FT + all_NBA + FTA + Pos, 
             data = kt_data)

## reduced further to
kt_lm3 <- lm(formula = Actual ~ yrs_exp + num_teams_played + GS. + 
                                NBA_titles + num_teams_1920 + PF + DRB, 
             data = kt_data)
```

```{r}
## analysis
kt_lm1 %>% 
  summary()
kt_lm2 %>% 
  summary()
kt_lm3 %>% 
  summary()
```

```{r}
## boxcox plots
kt_bxcx2 <- boxcox(kt_lm2, seq(-1, 1))
kt_bxcx3 <- boxcox(kt_lm3, seq(-1, 1))

## finding optimal lambda to transform y-variables
kt_bxcx2$x[which.max(kt_bxcx2$y)]
kt_bxcx3$x[which.max(kt_bxcx3$y)]
```

```{r}
## transforming y, comparing models
# suggested regression from stepwise, transformed
kt_lm4 <- lm(formula = Actual^(1/3) ~ num_accolades + PTS + yrs_exp + num_teams_played + 
                                      GS. + NBA_titles + num_teams_1920 + PF + DRB + GS + 
                                      X2P. + FT + all_NBA + FTA + Pos, 
             data = kt_data)

# suggested regression, reduced+transformed
kt_lm5 <- lm(Actual^(31/99) ~ yrs_exp + num_teams_played + GS. + 
                              NBA_titles + num_teams_1920 + PF + DRB, 
             data = kt_data)

## analysis
kt_lm4 %>% 
  summary()
kt_lm5 %>% 
  summary()
```

```{r}
## without PF (personal fouls), shown to be insignificant
kt_lm6 <- lm(Actual ~ yrs_exp + num_teams_played + GS. + NBA_titles + 
                      num_teams_1920 + DRB, data = kt_data)
boxcox(kt_lm6)$x[which.max(boxcox(kt_lm6)$y)]
## power of 0.222222
kt_lm7 <- lm(Actual^(2/9) ~ GS. + FT + yrs_exp + num_teams_played, data = kt_data)
kt_lm7 %>% 
  summary()

## interactive variable
kt_lm8 <- lm(Actual ~ yrs_exp * num_teams_played + GS. + NBA_titles + 
                      num_teams_1920 + DRB, data = kt_data)
boxcox(kt_lm8)$x[which.max(boxcox(kt_lm8)$y)]
## power of 0.262626
kt_lm9 <- lm(Actual^(26/99) ~ yrs_exp * num_teams_played + GS. + NBA_titles + 
                              num_teams_1920 + DRB, data = kt_data)
kt_lm9 %>% 
  summary()
```

```{r}
## reduced dataset for further analysis
kt_data1 <- sapply(subset(kt_data, select = c(performance_stats, other_perf, 
                                              other_stats) %>% unique()), as.numeric) %>% 
              data.frame()
kt_data2 <- sapply(subset(kt_data, select = c(performance_stats, performance_stats_pct, 
                                              performance_stats_atmpt, other_perf, other_stats) %>% 
                                                unique()), as.numeric) %>% 
              data.frame()

## PCA
# first PCA, more reduced dataset
kt_pca1 <- prcomp(kt_data1, 
                  scale = TRUE)

# 95% of variance explained by first 14 PCAs
## after 14 PCAs, each successive PCA only explains <1% of additional variance
# 90% of variance explained by first 10 PCAs
## after 10 PCAs, each successive PCA only explains <2% of additional variance
# 75% of variance explained by first 6 PCAs
## after 6 PCAs, each successive PCA only explains <4% of additional variance

cumsum(kt_pca1$sdev^2 / sum(kt_pca1$sdev^2))

kt_pca1$rotation[, 1:14] %>% round(3) %>% abs()

# second PCA, less reduced dataset
kt_pca2 <- prcomp(kt_data2, 
                  scale = TRUE)

# 95% of variance explained by first 16 PCAs
## after 16 PCAs, each successive PCA only explains <1% of additional variance
# 90% of variance explained by first 12 PCAs
## after 12 PCAs, each successive PCA only explains <2% of additional variance
# 75% of variance explained by first 6 PCAs
## after 6 PCAs, each successive PCA only explains <4% of additional variance

cumsum(kt_pca2$sdev^2 / sum(kt_pca2$sdev^2))

kt_pca2$rotation[, 1:14] %>% round(3) %>% abs()
```

# Principal Component Analysis

Principal Component Analysis (PCA) was not a very useful method in the determination of an effective predictive model for NBA player salary; PCA was an ultimately ineffective method, as it required an excessive number of principal components to explain a reasonably high amount of variance. In both the restricted and unrestricted datasets, at least 10-12 PCAs were necessary to explain just 90% of the variance in the data.

```{r}
## lasso and  ridge regression
set.seed(19462)
nba_x <- model.matrix(Actual ~ ., kt_data2)[, -1]
nba_y <- kt_data2$Actual
# select a random sample
train <- sample(1:nrow(nba_x), nrow(nba_x) / 2)
# split data into testing
test_x <- (-train)
test_y <- nba_y[test_x]

# using CV to find the optimal lambda based on training set
set.seed(72347)

# fitting ridge regression
nba_cv <- cv.glmnet(nba_x[train, ],
                    nba_y[train],
                    alpha = 0)

NBA_ridge <- glmnet(nba_x[train, ],
                    nba_y[train],
                    alpha = 0,
                    lambda = nba_cv$lambda.min,
                    thresh = 1e-14)
# test MSE with optimal lambda
NBA_pred <- predict(NBA_ridge,
                    s = nba_cv$lambda.min,
                    newx = nba_x[test_x,])

# fitting lasso regression
nba_cv1 <- cv.glmnet(nba_x[train, ],
                     nba_y[train],
                     alpha = 1)

NBA_lasso <- glmnet(nba_x[train, ],
                    nba_y[train],
                    alpha = 1,
                    lambda = nba_cv1$lambda.min,
                    thresh = 1e-14)
# test MSE with optimal lambda
NBA_pred1 <- predict(NBA_lasso,
                     s = nba_cv$lambda.min,
                     newx = nba_x[test_x,])

# fitting OLS
NBA_OLS <- glmnet(nba_x[train, ],
                  nba_y[train],
                  alpha = 0,
                  lambda = 0,
                  thresh = 1e-14)

# test MSE with optimal lambda
NBA_pred2 <- predict(NBA_OLS,
                     s = 0,
                     newx = nba_x[test_x,])

# test MSEs, compare with previous models
## use predict() with some of the previous regressions to compare MSEs
mean((NBA_pred - test_y)^2) # ridge
mean((NBA_pred1 - test_y)^2) # lasso
mean((NBA_pred2 - test_y)^2) # OLS
mean((predict(lm(formula = Actual ~ GS. + DRB + PF + yrs_exp + num_accolades + 
                                    NBA_titles + num_teams_1920 + num_teams_played, 
              data = kt_data[train, ]), 
              newx = nba_x[test_x,]) - test_y)^2) # first reduced model (lm2)
mean((predict(lm(formula = Actual ~ yrs_exp + num_teams_played + GS. + 
                                    NBA_titles + num_teams_1920 + PF + DRB, 
              data = kt_data[train, ]), 
              newx = nba_x[test_x,]) - test_y)^2) # reduced model, based off stepwise regression (lm3)
mean((predict(lm(formula = Actual^(31/99) ~ yrs_exp + num_teams_played + GS. + 
                                            NBA_titles + num_teams_1920 + PF + DRB, 
              data = kt_data[train, ]), 
              newx = nba_x[test_x,]) - test_y)^2) # best model after initial y-transformation (lm5)
mean((predict(lm(formula = Actual^(2/9) ~ GS. + FT + yrs_exp + num_teams_played, 
              data = kt_data[train, ]), 
              newx = nba_x[test_x,]) - test_y)^2) # further reduced model, y-transformation (lm7)
mean((predict(lm(formula = Actual^(26/99) ~ yrs_exp * num_teams_played + GS. + NBA_titles + num_teams_1920 + DRB, 
              data = kt_data[test_x,]), 
              newx = nba_x[test_x,]) - test_y)^2) # best model, y-transformation and interaction variable (lm9)
```

```{r}
## function to return PRESS
PRESS <- function(lm) {
  ## calculate the predictive residuals
  pr <- residuals(lm) / (1 - lm.influence(lm)$hat)
  ## calculate the PRESS
  PRESS <- sum(pr^2)
  return(PRESS)
}
```

```{r}
## analyzing best models
# fifth model
acf(kt_lm5$residuals)
qqnorm(kt_lm5$residuals)
qqline(kt_lm5$residuals, col = 'red')

plot(kt_lm5$fitted.values, kt_lm5$residuals, main = 'Residual Plot, Fifth Model')
abline(h = 0, col= 'red')
vif(kt_lm5)
PRESS(kt_lm5)
```

```{r}
# eighth model
acf(kt_lm7$residuals)
qqnorm(kt_lm7$residuals)
qqline(kt_lm7$residuals, col = 'red')

plot(kt_lm7$fitted.values, kt_lm7$residuals, main = 'Residual Plot, Eighth Model')
abline(h = 0, col= 'red')
vif(kt_lm7)
PRESS(kt_lm7)
```

```{r}
# tenth model
acf(kt_lm8$residuals)
qqnorm(kt_lm8$residuals)
qqline(kt_lm8$residuals, col = 'red')

plot(kt_lm8$fitted.values, kt_lm8$residuals, main = 'Residual Plot, Tenth Model')
abline(h = 0, col= 'red')
vif(kt_lm8)
PRESS(kt_lm8)
```

## Deciding On A Model

After the use of shrinking methods, and comparing their results to the various regression models previously created, the y-transformed models fare the best, with the lowest MSE values when used on split training and testing data.

While the fifth model, `Actual^(31/99) ~ yrs_exp + num_teams_played + GS. + NBA_titles + num_teams_1920 + PF + DRB` had the lowest MSE during testing, it was only marginally lower than the other two y-transformed models (the seventh and eighth models). I would settle on the seventh model, using both a y-transformation and more reduced variables. The seventh model had overall the lowest residual standard error, a strong adjusted R-squared value, and the lowest predicted residual error sum of squares (PRESS) statistic.

# Fifth Model

```
lm(formula = Actual^(31/99) ~ yrs_exp + num_teams_played + GS. + 
    NBA_titles + num_teams_1920 + PF + DRB, data = kt_data)

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)      68.772609   3.762377  18.279  < 2e-16 ***
yrs_exp           8.106399   0.548001  14.793  < 2e-16 ***
num_teams_played -8.685130   1.158136  -7.499  3.4e-13 ***
GS.              37.957718   4.763240   7.969  1.3e-14 ***
NBA_titles       -5.984639   2.917020  -2.052 0.040779 *  
num_teams_1920    5.517161   2.466206   2.237 0.025764 *  
PF               -0.005576   0.038728  -0.144 0.885581    
DRB               0.070010   0.018371   3.811 0.000158 ***

Residual standard error: 30.25 on 454 degrees of freedom
Multiple R-squared:  0.6191,	Adjusted R-squared:  0.6133 
F-statistic: 105.4 on 7 and 454 DF,  p-value: < 2.2e-16
PRESS statistic: 436560
```

# Seventh Model

```
lm(formula = Actual^(2/9) ~ GS. + FT + yrs_exp + num_teams_played, 
    data = kt_data)

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)      21.614465   0.509838  42.395  < 2e-16 ***
GS.               6.028513   0.786875   7.661 1.11e-13 ***
FT                0.023240   0.003177   7.315 1.17e-12 ***
yrs_exp           1.273308   0.090857  14.014  < 2e-16 ***
num_teams_played -1.181030   0.193846  -6.093 2.36e-09 ***

Residual standard error: 5.383 on 457 degrees of freedom
Multiple R-squared:  0.6073,	Adjusted R-squared:  0.6039 
F-statistic: 176.7 on 4 and 457 DF,  p-value: < 2.2e-16
PRESS statistic: 13651.72
```

# Eighth Model

```
lm(formula = Actual ~ yrs_exp * num_teams_played + GS. + NBA_titles + 
    num_teams_1920 + DRB, data = kt_data)

Residuals:
      Min        1Q    Median        3Q       Max 
-24907676  -3157517    -30735   2394214  17145906 

Coefficients:
                         Estimate Std. Error t value Pr(>|t|)    
(Intercept)              -4172171     773248  -5.396 1.10e-07 ***
yrs_exp                   2173550     133574  16.272  < 2e-16 ***
num_teams_played          -101449     336748  -0.301  0.76335    
GS.                       5943967     834852   7.120 4.25e-12 ***
NBA_titles               -1281241     520760  -2.460  0.01425 *  
num_teams_1920             721074     430107   1.676  0.09433 .  
DRB                          7509       2317   3.240  0.00128 ** 
yrs_exp:num_teams_played  -216486      31206  -6.937 1.38e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5397000 on 454 degrees of freedom
Multiple R-squared:  0.6223,	Adjusted R-squared:  0.6165 
F-statistic: 106.9 on 7 and 454 DF,  p-value: < 2.2e-16
PRESS statistic: 1.39803e+16
```


```{r}
## finding outliers
## using tenth model, with interaction variable
kt_lm7res <- rstandard(kt_lm7)
kt_lm7studentres <- rstudent(kt_lm7)

# plotting
par(mfrow = c(1, 3))
plot(kt_lm7$fitted.values, kt_lm7$residuals, main = "Residuals")
plot(kt_lm7$fitted.values, kt_lm7res, main = "Studentized Residuals")
plot(kt_lm7$fitted.values, kt_lm7studentres, main = "Externally Studentized Residuals")
kt_lm7studentres[abs(kt_lm7studentres) > qt(1 - 0.05 / (2 * 462), 457)]
```

```{r}
## players with high leverage
NBA_leverage <- lm.influence(kt_lm7)$hat
NBA_leverage <- NBA_leverage[NBA_leverage > 2 * 4 / 462]
NBA_leverage

# plotting
plot(NBA_leverage, main = "Leverages", ylim = c(0, 0.2))
abline(h = 2 * 4 / 462, col = "red")
```

```{r}
# DFFITS
NBA_DFFITS <- dffits(kt_lm7)
NBA_DFFITS <- NBA_DFFITS[abs(NBA_DFFITS) > 2 * sqrt(4 / 462)]
NBA_DFFITS
```


```{r}
# Cook's distance
NBA_COOKS <- cooks.distance(kt_lm7)
NBA_COOKS <- NBA_COOKS[NBA_COOKS > qf(0.95, 4, 462 - 4)]
NBA_COOKS
```

```{r}
## indexes for players who are outliers in terms of salary
# measured by both leverage and DFFITS
outlier_index <- c(names(NBA_DFFITS), names(NBA_leverage)) %>% 
                  unique() %>% 
                  as.numeric() %>% 
                  sort()

## apply model to outlier players to find predicted salary
outlier_pred <- predict(kt_lm7, newx = kt_data, type = 'response', level = 0.95)

## as response variable has y-transformation, find the base for actual predicted salary figures
outlier_pred <- exp(log(outlier_pred) / (2/9))

## create data frame with player names, actual salary, predicted salary, and residuals
pred_results <- data.frame(Player = kt_data$Player, 
                           Actual_Salary = kt_data$Actual, 
                           Predicted_Salary = outlier_pred)

## residual equals the difference between Actual and Predicted salaries
pred_results$Residuals <- pred_results$Actual_Salary - pred_results$Predicted_Salary

## percent (%) value the player is over or underpaid by
pred_results$Salary_DeltaPCT <- ((pred_results$Residuals / pred_results$Actual_Salary) * 100) %>% round(2)

## string for easy output of results
pred_results$String <- ifelse(pred_results$Salary_DeltaPCT < 0, 
                              sprintf('%s was underpaid by $%s (%.1f%%)', 
                                      pred_results$Player, 
                                      abs(pred_results$Residuals) %>% 
                                        round(2) %>% 
                                        format(big.mark = ',', nsmall = 2, trim = TRUE), 
                                      pred_results$Salary_DeltaPCT), 
                              sprintf('%s was overpaid by $%s (%.1f%%)', 
                                      pred_results$Player, 
                                      abs(pred_results$Residuals) %>% 
                                        round(2) %>% 
                                        format(big.mark = ',', nsmall = 2, trim = TRUE), 
                                      pred_results$Salary_DeltaPCT))
```


```{r}
## top ten most overpaid players, by pct
head(arrange(pred_results[outlier_index, ], desc(Salary_DeltaPCT)), n = 10)$String
```


```{r}
## top ten most underpaid players, by pct
head(arrange(pred_results[outlier_index, ], Salary_DeltaPCT), n = 10)$String
```


```{r}
## top ten most overpaid players, by total residual
head(arrange(pred_results[outlier_index, ], desc(Residuals)), n = 10)$String
```


```{r}
## top ten most underpaid players, by total residual
head(arrange(pred_results[outlier_index, ], Residuals), n = 10)$String
```

## Finding The Most Overpaid And Underpaid Players In The NBA

When applying our predictive model and searching for outliers in the dataset, these were the most overpaid and underpaid NBA athletes of the 2019-2020 season:

# Most Overpaid

Players most overpaid, by percentage of their salary.

1. Chandler Parsons was overpaid by $21,663,000.52 (-86.3%) 
2. Allen Crabbe was overpaid by $15,309,297.79 (-85.9%)    
3. Evan Turner was overpaid by $14,959,720.04 (-80.4%)      
4. Dewayne Dedmon was overpaid by $10,430,128.05 (-78.2%)  
5. Otto Porter was overpaid by $20,905,632.12 (-76.7%)      
6. Wayne Ellington was overpaid by $5,701,677.55 (-73.0%)  
7. Ish Smith was overpaid by $3,957,484.55 (-67.6%)         
8. Courtney Lee was overpaid by $8,424,653.12 (-66.0%)     
9. Justin Holiday was overpaid by $3,145,407.33 (-66.0%)    
10. Kent Bazemore was overpaid by $12,448,026.90 (-64.6%)  

**Honorable Mention:** 
Players most overpaid, in total dollars (if not previously mentioned above).

* Tobias Harris was overpaid by $19,295,968.78 (-62.2%)
* D'Angelo Russell was overpaid by $16,202,238.54 (-59.4%)
* Nicolas Batum was overpaid by $16,026,047.29 (-62.7%)
* Joel Embiid was overpaid by $11,831,148.68 (-43.0%)

# Most Underpaid

Players most underpaid, by percentage of their salary.

1. Jamal Crawford was underpaid by $10,369,066.71 (-3578.0%)
2. Lance Thomas was underpaid by $5,576,585.48 (-2127.2%)
3. Luguentz Dort was underpaid by $2,781,022.35 (-1786.8%)
4. Luc Mbah a Moute was underpaid by $4,035,528.06 (-1392.5%)
5. Carmelo Anthony was underpaid by $29,860,499.33 (-1383.0%)
6. Corey Brewer was underpaid by $3,369,160.12 (-1162.6%)
7. Udonis Haslem was underpaid by $21,435,764.49 (-835.8%)
8. Alec Burks was underpaid by $13,558,585.79 (-584.4%)
9. Vince Carter was underpaid by $14,966,920.64 (-583.6%)
10. Tyler Zeller was underpaid by $1,242,329.25 (-505.7%)

**Honorable Mention:** 
Players most underpaid, in total dollars (if not previously mentioned above).

* James Harden was underpaid by $32,137,356.33 (-85.0%)
* LeBron James was underpaid by $14,749,107.48 (-39.4%)
* J.J. Barea was underpaid by $11,537,178.68 (-449.8%)
* Lou Williams was underpaid by $10,405,363.78 (-130.1%)
* Trae Young was underpaid by $10,261,123.90 (-163.6%)

```{r}
# Jordan
# automated search procedures

# !! Do we want to split the data and do crosswise regression?
##intercept only model
regnull <- lm(jh_data$Actual~1, data=jh_data)

##model with all predictors
regfull <- lm(Actual~.-Player-Guaranteed-url, data=jh_data)

# step_reg <- step(regnull, scope=list(lower=regnull, upper=regfull), direction="both")
```

```{r}
# model evaluation see chapters 5 and 7
library(faraway)
jh_result<-lm(Actual ~ num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB + GS + eFG. + 
    all_NBA, data = jh_data)

summary(jh_result)
anova(jh_result)

vif(jh_result)
# multicolinearity appears to be an issue with GS and potentially eFG., num_teams_1920, and GS.
#correlation between variables?

vcov(jh_result)[,c('GS','eFG.',"num_teams_1920","GS.")]
```
From the VCOV table, looking at rows specifically:
GS - 
```{r}
# let's try dropping GS (highest VIF) and eFG. (also a major one) and see how things change
jh_result.2<-lm(Actual ~ num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB + 
    all_NBA, data = jh_data)
summary(jh_result.2)
# Adj R2 is about the same
anova(jh_result.2)
vif(jh_result.2)
# much better VIF
```

```{r}
#partial f test, if this has p>0.05 we reject null and keep full model.
# remove all_NBA
reduced.1 <- lm(Actual~num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB, data = jh_data)
anova(reduced.1,jh_result.2) 
summary(reduced.1)

# we fail to reject the null so let's go with the smaller model

```


```{r}
library(MASS)
# Do we need to transform predictor? If lambda=1 is in the confidence interval we do not need a transformation, if 0 is in CI then us ln(y), otherwise use y^lambda (see pg 17 in notes)
boxcox(reduced.1)
# check distribution of residuals
plot(residuals(reduced.1))
abline(h=0,col='red')
# check normal distribution
qqnorm(reduced.1$residuals)
qqline(reduced.1$residuals, col="red")
# autocorrelation
acf(reduced.1$residuals, main="ACF of Residuals")


#--------------------
# Transformation
jh_data$Actual.t <- jh_data$Actual^(1/4)
reduced.t<- lm(Actual.t~num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB, data = jh_data)
summary(reduced.t)
anova(reduced.t)

#check assumptions on transformed data:
boxcox(reduced.t)
# check distribution of residuals
plot(residuals(reduced.t))
abline(h=0,col='red')
# check normal distribution
qqnorm(reduced.t$residuals)
qqline(reduced.t$residuals, col="red")
# autocorrelation
acf(reduced.t$residuals, main="ACF of Residuals")
# lag of 24ish is above ci, do we need to worry about this?

```
```{r}
stepBoth <- lm(jh_data$Actual.t ~ yrs_exp + num_teams_played + GS. + 
    drafted  + G + Pos + AST + draft_num + DRB + twitter_followers + 
    FGA + PF + injury_yrs, data=jh_data)
stepBoth
summary(stepBoth)
```

Questions:
1. Is it a problem if we don't have categorical variables? (no but we wil anyway)
2. Is interaction only for categorical variables (Answer: No, but should we worry about it?  Are there likely candidates? Should we simplify?)
(a) How do we find a good interaction with just 1 categorical variable?
3. Do we want to error on the side of more predictors (less bias) or fewer predictors (less variance)?
4. Should we do cross validation?
5. Should we check AIC/BIC/Press statistic (43)
6. Test predictive ability?
7. Should we try to get better linear fits by dealing with outliers? (apparenty we don't have to check categorical predictors for outliers (see discussion notes))
8. (partial regression plot, pg. 48, module 8)


```{r}
jh_data$Pos<-factor(jh_data$Pos)
is.factor(jh_data$Pos)

levels(jh_data$Pos)
contrasts(jh_data$Pos)

# equality of variance assumption test.  H0 variances are the same, HA variances are not the same for all classes
library(lawstat)
levene.test(jh_data$Actual.t,jh_data$Pos)
# greater than 0.05 so we reject H0 and we are in good shape

a1<-subset(jh_data,Pos=="C")
a2<-subset(jh_data,Pos=="C-PF")
a3<-subset(jh_data,Pos=="PF")
a4<-subset(jh_data,Pos=="PF-C")
a5<-subset(jh_data,Pos=="PF-SF")
a6<-subset(jh_data,Pos=="PG")
a7<-subset(jh_data,Pos=="PG-SG")
a8<-subset(jh_data,Pos=="SF")
a9<-subset(jh_data,Pos=="SF-C")
a10<-subset(jh_data,Pos=="SF-PF")
a11<-subset(jh_data,Pos=="SF-SG")
a12<-subset(jh_data,Pos=="SG")
a13<-subset(jh_data,Pos=="SG-PG")
a14<-subset(jh_data,Pos=="SG-SF")


reg1<-lm(Actual.t~PTS,data=a1)
reg2<-lm(Actual.t~PTS,data=a2)
reg3<-lm(Actual.t~PTS,data=a3)
reg4<-lm(Actual.t~PTS,data=a4)
reg5<-lm(Actual.t~PTS,data=a5)
reg6<-lm(Actual.t~PTS,data=a6)
reg7<-lm(Actual.t~PTS,data=a7)
reg8<-lm(Actual.t~PTS,data=a8)
reg9<-lm(Actual.t~PTS,data=a9)
reg10<-lm(Actual.t~PTS,data=a10)
reg11<-lm(Actual.t~PTS,data=a11)
reg12<-lm(Actual.t~PTS,data=a12)
reg13<-lm(Actual.t~PTS,data=a13)
reg14<-lm(Actual.t~PTS,data=a14)

plot(jh_data$PTS,jh_data$Actual.t, main="Actual income against PTS, by Pos")
points(a2$PTS,a2$Actual.t, pch=2, col="red")
points(a3$PTS,a3$Actual.t, pch=3, col="blue")
points(a4$PTS,a4$Actual.t, pch=4, col="green")
points(a5$PTS,a5$Actual.t, pch=5, col="orange")
points(a6$PTS,a6$Actual.t, pch=6, col="purple")
points(a7$PTS,a7$Actual.t, pch=7, col="pink")
points(a8$PTS,a8$Actual.t, pch=8, col="seagreen2")
points(a9$PTS,a9$Actual.t, pch=9, col="salmon3")
points(a10$PTS,a10$Actual.t, pch=10, col="royalblue4")
points(a11$PTS,a11$Actual.t, pch=11, col="rosybrown4")
points(a12$PTS,a12$Actual.t, pch=12, col="skyblue2")
points(a13$PTS,a13$Actual.t, pch=13, col="slateblue2")
points(a14$PTS,a14$Actual.t, pch=14, col="slategray2")

# Note: I've commented out lines that were failing because they only have one or two values
abline(reg1,lty=1)
# abline(reg2,lty=2, col="red")
abline(reg3,lty=3, col="blue")
abline(reg4,lty=4, col="green")
abline(reg5,lty=5, col="orange")
abline(reg6,lty=6, col="purple")
# abline(reg7,lty=7, col="pink")
abline(reg8,lty=8, col="seagreen2")
# abline(reg9,lty=9, col="salmon3")
abline(reg10,lty=10, col="royalblue4")
# abline(reg11,lty=11, col="rosybrown4")
abline(reg12,lty=12, col="skyblue2")
abline(reg13,lty=13, col="slateblue2")
# abline(reg14,lty=14, col="slategray2")

legend("right", c("C","C-PF","PF","PF-C","PF-SF","PG","PG-SG","SF","SF-C","SF-PF","SF-SG","SG","SG-PG","SG-SF"), lty=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14), pch=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14), col=c("black","red","blue","green","orange","purple","pink","seagreen2","salmon3","royalblue4","rosybrown4","skyblue2","slateblue2","slategray2"))

# some of these lines are a little nuts (PF-SF, SG-PG), these should be collapsed into other fields

# run a boxplot to see if it makes sense to combine others of these
boxplot(jh_data$Actual.t~jh_data$Pos, main="Boxplot of Actual Pay by Position")

# colapse classes
# create new collapsed class
jh_data$Pos.colapsed <- jh_data$Pos

# Eliminate all the double position classes. When no clear preference in the box plot pick the primary position.

# C-PF doesn't really fit anywhere, put with C
jh_data$Pos.colapsed[which(jh_data$Pos == "C-PF")] = "C"
# PF-C closer to C than PF. 
jh_data$Pos.colapsed[which(jh_data$Pos == "PF-C")] = "C"
# PF-SF is unique, we will keep that one around
# PG-SG we can group with PG (no natural match)
jh_data$Pos.colapsed[which(jh_data$Pos == "PG-SG")] = "PG"
# SF-C we will group with SF, no natural match
jh_data$Pos.colapsed[which(jh_data$Pos == "SF-C")] = "SF"
# SF-PF seems like a more natural match with SF
jh_data$Pos.colapsed[which(jh_data$Pos == "SF-PF")] = "SF"
# SF-SG seems like a better match with SF
jh_data$Pos.colapsed[which(jh_data$Pos == "SF-SG")] = "SF"
# SG-PG we will go with SG, no obvious match
jh_data$Pos.colapsed[which(jh_data$Pos == "SG-PG")] = "SF"
# SG-SF we will go with SG, no obvious match
jh_data$Pos.colapsed[which(jh_data$Pos == "SG-SF")] = "SF"

boxplot(jh_data$Actual.t~jh_data$Pos.colapsed, main="Boxplot of Actual Pay by Colapsed Position")

# Check the new model
reduced.t2<-lm(jh_data$Actual.t ~ yrs_exp + num_teams_played + GS. +
    drafted  + G + Pos.colapsed + AST + draft_num + DRB + twitter_followers +
    FGA + PF + injury_yrs, data=jh_data)
summary(reduced.t2)

# equality of variance assumption test.  H0 variances are the same, HA variances are not the same for all classes
library(lawstat)
levene.test(jh_data$Actual.t,jh_data$Pos.colapsed)
# greater than 0.05 so we reject H0 and we are in good shape

# replot the graph, look for nonlinear relationships (not paralel lines) add in interaction and compare partial f tests
b1<-subset(jh_data,Pos.colapsed=="C")
b2<-subset(jh_data,Pos.colapsed=="PF")
b3<-subset(jh_data,Pos.colapsed=="PF-SF")
b4<-subset(jh_data,Pos.colapsed=="PG")
b5<-subset(jh_data,Pos.colapsed=="SF")
b6<-subset(jh_data,Pos.colapsed=="SG")


reg1<-lm(Actual.t~PTS,data=b1)
reg2<-lm(Actual.t~PTS,data=b2)
reg3<-lm(Actual.t~PTS,data=b3)
reg4<-lm(Actual.t~PTS,data=b4)
reg5<-lm(Actual.t~PTS,data=b5)
reg6<-lm(Actual.t~PTS,data=b6)

plot(jh_data$PTS,jh_data$Actual.t, main="Actual income against PTS, by Pos")
points(b2$PTS,b2$Actual.t, pch=2, col="red")
points(b3$PTS,b3$Actual.t, pch=3, col="blue")
points(b4$PTS,b4$Actual.t, pch=4, col="green")
points(b5$PTS,b5$Actual.t, pch=5, col="orange")
points(b6$PTS,b6$Actual.t, pch=6, col="purple")

abline(reg1,lty=1)
abline(reg2,lty=2, col="red")
abline(reg3,lty=3, col="blue")
abline(reg4,lty=4, col="green")
abline(reg5,lty=5, col="orange")
abline(reg6,lty=6, col="purple")

legend("right", c("C","PF","PF-SF","PG","SF","SG"), lty=c(1,2,3,4,5,6), pch=c(1,2,3,4,5,6), col=c("black","red","blue","green","orange","purple"))


# lines are not paralel indicating a posible interaction.  Let's see what that model looks like.
reduced.t3<-lm(jh_data$Actual.t ~ yrs_exp + num_teams_played + GS. +
    drafted  + G * Pos.colapsed + AST + draft_num + DRB + twitter_followers +
    FGA + PF + injury_yrs, data=jh_data)
summary(reduced.t3)

# partial f test, should we keep the interaction?
anova(reduced.t3,reduced.t2)
# fail to reject the null, we will remove the interaction
```

