---
title: "Data Analysis"
author: "Jordan Hiatt"
date: "12/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(corrplot)
library(GGally)
library(PerformanceAnalytics)
library(plotly)
```


```{r}
# Jordan

# save the data separately
jh_data <- read.csv(file = 'NBA_salary_stats.csv', row.names = 1, header = TRUE)

# Correlation plot (Khoi's code)
corrplot(cor(jh_data[, -c(1, 3, 4, 30, 38, 45)],
# corrplot(cor(jh_data[c(2,10)],
        use = 'complete.obs'),
        method = 'circle',
        type = 'upper')

# Just correlation of all vars with y
correlations<-cor(jh_data[, -c(1, 3, 4, 30, 38, 45)])[ ,1]

# save the highly correlated variables (above .5 is my threshold)
high_cor <- Filter(function(x) x>.5,correlations)

# remove the y variable
high_cor<-high_cor[2:length(high_cor)]

# Loop through all highly correlated vars to get regressions and plots
for (c in names(high_cor)) {
  # get the model for each highly correlated predictor
  l<-lm(jh_data$Actual~jh_data[[c]])
  
  # plot the model with the regression line
  plot(jh_data$Actual~jh_data[[c]], main=c)
  abline(l,col='red')
  
  # residual plots
  # these residual plots look really good. The only note is that the higher end errors are a little bigger making the center above the clear mass of residuals in most plots.
  plot(residuals(l), main = c)
  abline(h=0)
}

# see chap 8 and chap 3
```

```{r}
# exploratory data analysis
## reading in data
kt_data <- read.csv(file = 'NBA_salary_stats.csv', 
                    row.names = 1, 
                    header = TRUE)
```

```{r}
# drop Guaranteed salary in exchange for using Actual pay when predicting salary (index = 3rd column)
### correlation plots, 2019-20 season performance stats
## correlation plots for total point scoring
other_perf <- c('ORB', 'DRB', 'AST', 'STL', 'BLK', 'TOV', 'PF')
performance_stats <- c('Actual', 'MP', 'GS', 'X2P', 'X3P', 'FT')
corrplot(cor(subset(kt_data, select = c(performance_stats, other_perf)),
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = performance_stats)) %>% 
  pairs()
```

```{r}
## correlation plots with accuracy (e.g. 3-point percentage)
performance_stats_pct <- c('Actual', 'MP', 'GS.', 'X2P.', 'X3P.', 'FT.')
corrplot(cor(subset(kt_data, select = c(performance_stats_pct, other_perf)),
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = performance_stats_pct)) %>% 
  pairs()
```

```{r}
## correlation plots with attempts (e.g. 3-point attempts)
performance_stats_atmpt <- c('Actual', 'MP', 'G', 'X2PA', 'X3PA', 'FTA')
corrplot(cor(subset(kt_data, select = c(performance_stats_atmpt, other_perf)),
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = performance_stats_atmpt)) %>% 
  pairs()
```

```{r}
### correlation plots, overall career statistics
# e.g. mid-season trades, experience, age, etc.
other_stats <- c('Actual', 'yrs_exp', 'age_yrs', 'draft_num', 
                 'num_accolades', 'NBA_titles', 'all_NBA', 'num_teams_1920', 
                 'mid_season_trades', 'num_yrs_current_team', 
                 'num_teams_played', 'injury_yrs', 'twitter_followers')
corrplot(cor(subset(kt_data, select = other_stats), 
             use = 'complete.obs'),
         method = 'circle',
         type = 'upper')

cor(subset(kt_data, select = other_stats)) %>% 
  pairs()
```

```{r}
## list variables most correlated to actual salary
cor(subset(kt_data, select = performance_stats))[,1]
# GS, X2P, FT are > 0.5
# X3P > 0.38
# MP is a decent correlator in all performance-related correlations
cor(subset(kt_data, select = performance_stats_pct))[,1]
# GS., X2P. are > 0.5
# MP still a decent correlator
cor(subset(kt_data, select = performance_stats_atmpt))[,1]
# FTA and X2PA are > 0.5
# X3PA > 0.39
# MP still a decent correlator
cor(subset(kt_data, select = other_stats))[,1]
# yrs_exp, num_accolades are > 0.5
# age_yrs, all_NBA, num_yrs_current_team, twitter_followers > 0.3
cor(subset(kt_data, select = c('Actual', other_perf)))[,1]
# DRB, AST, TOV are > 0.5
# ORB, STL, BLK, and PF are > 0.3
```

```{r}
## regression with all variables but player name, guaranteed salary, link to url, and team
lm(Actual ~ ., data = kt_data[-c(1, 3, 30, 38)]) %>% 
  summary()
## regression with performance stats
lm(Actual ~ ., data = subset(kt_data, select = performance_stats)) %>% 
    summary()
## regression with performance stats, percentages
lm(Actual ~ ., data = subset(kt_data, select = performance_stats_pct)) %>% 
    summary()
## regression with performance stats, attempts
lm(Actual ~ ., data = subset(kt_data, select = performance_stats_atmpt)) %>% 
    summary()
## regression with other performance stats
lm(Actual ~ ., data = subset(kt_data, select = c('Actual', other_perf))) %>% 
    summary()
## regression with other stats
lm(Actual ~ ., data = subset(kt_data, select = other_stats)) %>% 
    summary()
```

```{r}
## all stats above
lm(Actual ~ ., data = subset(kt_data, select = c(performance_stats, performance_stats_pct, 
                                                 performance_stats_atmpt, other_perf, other_stats) %>% 
                                                  unique())) %>% 
    summary()

## regression with variables observed in correlation plots and previous regressions to be significant
lm(Actual ~ GS. + FT + yrs_exp + DRB + AST + TOV + 
            yrs_exp + num_accolades + num_teams_played, 
    data = kt_data) %>% 
      summary()

## further reduced regression, more significant
kt_lm1 <- lm(Actual ~ GS. + FT + yrs_exp + 
                      num_accolades + num_teams_played, 
             data = kt_data)
## stepwise
step(lm(formula = Actual ~ 1, data = kt_data[-c(1, 3, 30, 38)]), 
     scope = list(lower = lm(formula = Actual ~ 1, data = kt_data[-c(1, 3, 30, 38)]), 
                  upper = lm(formula = Actual ~ ., data = kt_data[-c(1, 3, 30, 38)])), 
     direction = 'both')

## suggested regression
kt_lm2 <- lm(formula = Actual ~ num_accolades + PTS + yrs_exp + num_teams_played + 
                        GS. + NBA_titles + num_teams_1920 + PF + DRB + 
                        GS + eFG. + all_NBA, 
             data = kt_data)

## reduced further to
kt_lm3 <- lm(formula = Actual ~ GS. + FT + yrs_exp + num_accolades + 
                        num_teams_played + num_teams_1920 + PF, 
                data = kt_data)
```

```{r}
## analysis
kt_lm1 %>% 
  summary()
kt_lm2 %>% 
  summary()
kt_lm3 %>% 
  summary()
```

```{r}
## boxcox plots
kt_bxcx1 <- boxcox(kt_lm1, seq(-1, 1))
kt_bxcx2 <- boxcox(kt_lm2, seq(-1, 1))
kt_bxcx3 <- boxcox(kt_lm3, seq(-1, 1))

## finding optimal lambda to transform y-variables
kt_bxcx1$x[which.max(kt_bxcx1$y)]
kt_bxcx2$x[which.max(kt_bxcx2$y)]
kt_bxcx3$x[which.max(kt_bxcx3$y)]
```

```{r}
## transforming y, comparing models
kt_lm4 <- lm(Actual^(1/3) ~ GS. + FT + yrs_exp + 
                      num_accolades + num_teams_played, 
             data = kt_data)

## suggested regression
kt_lm5 <- lm(Actual^(31/99) ~ num_accolades + PTS + yrs_exp + num_teams_played + 
                      GS. + NBA_titles + num_teams_1920 + PF + DRB + 
                      GS + eFG. + all_NBA, 
             data = kt_data)

## reduced further to
kt_lm6 <- lm(Actual^(1/3) ~ GS. + FT + yrs_exp + num_accolades + 
                      num_teams_played + num_teams_1920 + PF, 
             data = kt_data)

## analysis
kt_lm4 %>% 
  summary()
kt_lm5 %>% 
  summary()
kt_lm6 %>% 
  summary()
anova(kt_lm4, kt_lm6)
```

```{r}
## without num_accolades, shown to be less significant
kt_lm7 <- lm(Actual ~ GS. + FT + yrs_exp + num_teams_played, data = kt_data)
boxcox(kt_lm7)$x[which.max(boxcox(kt_lm7)$y)]
## power of 0.262626
kt_lm8 <- lm(Actual^(26/99) ~ GS. + FT + yrs_exp + num_teams_played, data = kt_data)
kt_lm8 %>% 
  summary()

## interactive variable
kt_lm9 <- lm(Actual ~ GS. + FT + yrs_exp * num_teams_played, data = kt_data)
boxcox(kt_lm9)$x[which.max(boxcox(kt_lm9)$y)]
## power of 0.262626
kt_lm10 <- lm(Actual^(26/99) ~ GS. + FT + yrs_exp * num_teams_played, data = kt_data)
kt_lm10 %>% 
  summary()
```

```{r}
## reduced dataset for further analysis
kt_data1 <- sapply(subset(kt_data, select = c(performance_stats, other_perf, 
                                              other_stats) %>% unique()), as.numeric) %>% 
              data.frame()
kt_data2 <- sapply(subset(kt_data, select = c(performance_stats, performance_stats_pct, 
                                              performance_stats_atmpt, other_perf, other_stats) %>% 
                                                unique()), as.numeric) %>% 
              data.frame()

## PCA
# first PCA, more reduced dataset
kt_pca1 <- prcomp(kt_data1, 
                  scale = TRUE)

# 95% of variance explained by first 14 PCAs
## after 14 PCAs, each successive PCA only explains <1% of additional variance
# 90% of variance explained by first 10 PCAs
## after 10 PCAs, each successive PCA only explains <2% of additional variance
# 75% of variance explained by first 6 PCAs
## after 6 PCAs, each successive PCA only explains <4% of additional variance

cumsum(kt_pca1$sdev^2 / sum(kt_pca1$sdev^2))

kt_pca1$rotation[, 1:14] %>% round(3) %>% abs()

# second PCA, less reduced dataset
kt_pca2 <- prcomp(kt_data2, 
                  scale = TRUE)

# 95% of variance explained by first 16 PCAs
## after 16 PCAs, each successive PCA only explains <1% of additional variance
# 90% of variance explained by first 12 PCAs
## after 12 PCAs, each successive PCA only explains <2% of additional variance
# 75% of variance explained by first 6 PCAs
## after 6 PCAs, each successive PCA only explains <4% of additional variance

cumsum(kt_pca2$sdev^2 / sum(kt_pca2$sdev^2))

kt_pca2$rotation[, 1:14] %>% round(3) %>% abs()
```

# Principal Component Analysis

Principal Component Analysis (PCA) was not a very useful method in the determination of an effective predictive model for NBA player salary; PCA was an ultimately ineffective method, as it required an excessive number of principal components to explain a reasonably high amount of variance. In both the restricted and unrestricted datasets, at least 10-12 PCAs were necessary to explain just 90% of the variance in the data.

```{r}
## lasso and  ridge regression
set.seed(19462)
nba_x <- model.matrix(Actual ~ ., kt_data2)[, -1]
nba_y <- kt_data2$Actual
# select a random sample
train <- sample(1:nrow(nba_x), nrow(nba_x) / 2)
# split data into testing
test_x <- (-train)
test_y <- nba_y[test_x]

# using CV to find the optimal lambda based on training set
set.seed(72347)

# fitting ridge regression
nba_cv <- cv.glmnet(nba_x[train, ],
                    nba_y[train],
                    alpha = 0)

NBA_ridge <- glmnet(nba_x[train, ],
                    nba_y[train],
                    alpha = 0,
                    lambda = nba_cv$lambda.min,
                    thresh = 1e-14)
# test MSE with optimal lambda
NBA_pred <- predict(NBA_ridge,
                    s = nba_cv$lambda.min,
                    newx = nba_x[test_x,])

# fitting lasso regression
nba_cv1 <- cv.glmnet(nba_x[train, ],
                     nba_y[train],
                     alpha = 1)

NBA_lasso <- glmnet(nba_x[train, ],
                    nba_y[train],
                    alpha = 1,
                    lambda = nba_cv1$lambda.min,
                    thresh = 1e-14)
# test MSE with optimal lambda
NBA_pred1 <- predict(NBA_lasso,
                     s = nba_cv$lambda.min,
                     newx = nba_x[test_x,])

# fitting OLS
NBA_OLS <- glmnet(nba_x[train, ],
                  nba_y[train],
                  alpha = 0,
                  lambda = 0,
                  thresh = 1e-14)

# test MSE with optimal lambda
NBA_pred2 <- predict(NBA_OLS,
                     s = 0,
                     newx = nba_x[test_x,])

# test MSEs, compare with previous models
## use predict() with some of the previous regressions to compare MSEs
mean((NBA_pred - test_y)^2) # ridge
mean((NBA_pred1 - test_y)^2) # lasso
mean((NBA_pred2 - test_y)^2) # OLS
mean((predict(lm(formula = Actual ~ GS. + FT + yrs_exp + num_accolades + num_teams_played, 
              data = kt_data[train, ]), 
              newx = nba_x[test_x,]) - test_y)^2) # first reduced model (lm1)
mean((predict(lm(formula = Actual ~ GS. + FT + yrs_exp + num_accolades + num_teams_played + 
              num_teams_1920 + PF, data = kt_data[train, ]), 
              newx = nba_x[test_x,]) - test_y)^2) # reduced model, based off stepwise regression (lm3)
mean((predict(lm(formula = Actual^(31/99) ~ num_accolades + PTS + yrs_exp + 
              num_teams_played + GS. + NBA_titles + num_teams_1920 + PF + 
              DRB + GS + eFG. + all_NBA, data = kt_data[train, ]), 
              newx = nba_x[test_x,]) - test_y)^2) # best model after initial y-transformation (lm5)
mean((predict(lm(formula = Actual^(26/99) ~ GS. + FT + yrs_exp + num_teams_played, 
              data = kt_data[train, ]), 
              newx = nba_x[test_x,]) - test_y)^2) # further reduced model, y-transformation (lm8)
mean((predict(lm(formula = Actual^(26/99) ~ GS. + FT + yrs_exp * num_teams_played, 
              data = kt_data[test_x,]), 
              newx = nba_x[test_x,]) - test_y)^2) # best model, y-transformation and interaction variable (lm10)
```

## Deciding On A Model

After the use of shrinking methods, and comparing their results to the various regression models previously created, the y-transformed models fare the best, with the lowest MSE values when used on split training and testing data.

While the fifth model, `Actual^(31/99) ~ num_accolades + PTS + yrs_exp + num_teams_played + GS. + NBA_titles + num_teams_1920 + PF + DRB + GS + eFG. + all_NBA` had the lowest MSE during testing, it was only marginally lower than the other two y-transformed models (the eighth and tenth models). I would settle on the tenth model, using both a y-transformation and an interaction predictor variable. While it had weak signifiance on one predictor variable, the number of teams an NBA player has played on throughout his career (`num_teams_played`), it had overall the lowest residual standard error, with a strong adjusted R-squared value.

# Fifth Model

`Actual^(31/99) ~ num_accolades + PTS + yrs_exp + num_teams_played + GS. + NBA_titles + num_teams_1920 + PF + DRB + GS + eFG. + all_NBA`
`Coefficients:`
`                  Estimate Std. Error t value Pr(>|t|)    `
`(Intercept)      61.650599   3.993791  15.437  < 2e-16 ***`
`num_accolades     1.906608   1.648530   1.157  0.24807    `
`PTS               0.033886   0.005417   6.256 9.24e-10 ***`
`yrs_exp           7.440296   0.561973  13.240  < 2e-16 ***`
`num_teams_played -7.728382   1.126666  -6.860 2.30e-11 ***`
`GS.              62.645794   9.494565   6.598 1.18e-10 ***`
`NBA_titles       -5.505496   3.035832  -1.814  0.07042 .  `
`num_teams_1920    7.993471   5.909351   1.353  0.17684    `
`PF               -0.013111   0.041389  -0.317  0.75156    `
`DRB               0.051316   0.019311   2.657  0.00816 ** `
`GS               -0.742562   0.173599  -4.277 2.31e-05 ***`
`eFG.             -3.572819  11.472228  -0.311  0.75562    `
`all_NBA          -2.176728   1.613129  -1.349  0.17789    `

`Residual standard error: 28.67 on 449 degrees of freedom`
`Multiple R-squared:  0.6617,	Adjusted R-squared:  0.6527` 
`F-statistic: 73.19 on 12 and 449 DF,  p-value: < 2.2e-16`

#Eighth Model

`Actual^(26/99) ~ GS. + FT + yrs_exp + num_teams_played`
`Coefficients:`
`                  Estimate Std. Error t value Pr(>|t|)    `
`(Intercept)      37.754486   1.106871  34.109  < 2e-16 ***`
`GS.              13.138462   1.708326   7.691 9.05e-14 ***`
`FT                0.051700   0.006898   7.495 3.46e-13 ***`
`yrs_exp           2.822136   0.197252  14.307  < 2e-16 ***`
`num_teams_played -2.654906   0.420845  -6.309 6.66e-10 ***`

`Residual standard error: 11.69 on 457 degrees of freedom`
`Multiple R-squared:  0.6151,	Adjusted R-squared:  0.6117`
`F-statistic: 182.5 on 4 and 457 DF,  p-value: < 2.2e-16`

# Tenth Model

`Actual^(26/99) ~ GS. + FT + yrs_exp * num_teams_played`
`Coefficients:`
`                          Estimate Std. Error t value Pr(>|t|)    `
`(Intercept)              31.091803   1.430084  21.741  < 2e-16 ***`
`GS.                      11.854338   1.638041   7.237 1.96e-12 ***`
`FT                        0.047023   0.006606   7.118 4.27e-12 ***`
`yrs_exp                   4.111483   0.265067  15.511  < 2e-16 ***`
`num_teams_played          0.879632   0.650677   1.352    0.177    `
`yrs_exp:num_teams_played -0.433853   0.062907  -6.897 1.78e-11 ***`

`Residual standard error: 11.13 on 456 degrees of freedom`
`Multiple R-squared:  0.6514,	Adjusted R-squared:  0.6476`
`F-statistic: 170.4 on 5 and 456 DF,  p-value: < 2.2e-16`

```{r}
## finding outliers
## using tenth model, with interaction variable
kt_lm10res <- rstandard(kt_lm10)
kt_lm10studentres <- rstudent(kt_lm10)

# plotting
par(mfrow = c(1, 3))
plot(kt_lm10$fitted.values, kt_lm10$residuals, main = "Residuals")
plot(kt_lm10$fitted.values, kt_lm10res, main = "Studentized Residuals")
plot(kt_lm10$fitted.values, kt_lm10studentres, main = "Externally Studentized Residuals")
kt_lm10studentres[abs(kt_lm10studentres) > qt(1 - 0.05 / (2 * 462), 456)]
```

```{r}
## players with high leverage
NBA_leverage <- lm.influence(kt_lm10)$hat

# plotting
plot(NBA_leverage, main = "Leverages", ylim = c(0, 0.25))
abline(h = 2 * 5 / 462, col = "red")
NBA_leverage[NBA_leverage > 2 * 5 / 462]
```

```{r}
# DFFITS
NBA_DFFITS<- dffits(kt_lm10)
NBA_DFFITS[abs(NBA_DFFITS) > 2 * sqrt(5 / 562)]
```


```{r}
# Cook's distance
NBA_COOKS <- cooks.distance(kt_lm10)
NBA_COOKS[NBA_COOKS > qf(0.5, 3, 47 - 3)]
```

```{r}
# Jordan
# automated search procedures

# !! Do we want to split the data and do crosswise regression?
##intercept only model
regnull <- lm(jh_data$Actual~1, data=jh_data)

##model with all predictors
regfull <- lm(Actual~.-Player-Guaranteed-url, data=jh_data)

# step_reg <- step(regnull, scope=list(lower=regnull, upper=regfull), direction="both")
```

```{r}
# model evaluation see chapters 5 and 7
library(faraway)
jh_result<-lm(Actual ~ num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB + GS + eFG. + 
    all_NBA, data = jh_data)

summary(jh_result)
anova(jh_result)

vif(jh_result)
# multicolinearity appears to be an issue with GS and potentially eFG., num_teams_1920, and GS.
#correlation between variables?

vcov(jh_result)[,c('GS','eFG.',"num_teams_1920","GS.")]
```
From the VCOV table, looking at rows specifically:
GS - 
```{r}
# let's try dropping GS (highest VIF) and eFG. (also a major one) and see how things change
jh_result.2<-lm(Actual ~ num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB + 
    all_NBA, data = jh_data)
summary(jh_result.2)
# Adj R2 is about the same
anova(jh_result.2)
vif(jh_result.2)
# much better VIF
```

```{r}
#partial f test, if this has p>0.05 we reject null and keep full model.
# remove all_NBA
reduced.1 <- lm(Actual~num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB, data = jh_data)
anova(reduced.1,jh_result.2) 
summary(reduced.1)

# we fail to reject the null so let's go with the smaller model

```


```{r}
library(MASS)
# Do we need to transform predictor? If lambda=1 is in the confidence interval we do not need a transformation, if 0 is in CI then us ln(y), otherwise use y^lambda (see pg 17 in notes)
boxcox(reduced.1)
# check distribution of residuals
plot(residuals(reduced.1))
abline(h=0,col='red')
# check normal distribution
qqnorm(reduced.1$residuals)
qqline(reduced.1$residuals, col="red")
# autocorrelation
acf(reduced.1$residuals, main="ACF of Residuals")


#--------------------
# Transformation
jh_data$Actual.t <- jh_data$Actual^(1/4)
reduced.t<- lm(Actual.t~num_accolades + PTS + yrs_exp + num_teams_played + 
    GS. + NBA_titles + num_teams_1920 + PF + DRB, data = jh_data)
summary(reduced.t)
anova(reduced.t)

#check assumptions on transformed data:
boxcox(reduced.t)
# check distribution of residuals
plot(residuals(reduced.t))
abline(h=0,col='red')
# check normal distribution
qqnorm(reduced.t$residuals)
qqline(reduced.t$residuals, col="red")
# autocorrelation
acf(reduced.t$residuals, main="ACF of Residuals")
# lag of 24ish is above ci, do we need to worry about this?

```
```{r}
stepBoth <- lm(jh_data$Actual.t ~ yrs_exp + num_teams_played + GS. + 
    drafted  + G + Pos + AST + draft_num + DRB + twitter_followers + 
    FGA + PF + injury_yrs, data=jh_data)
stepBoth
summary(stepBoth)
```

Questions:
1. Is it a problem if we don't have categorical variables? (no but we wil anyway)
2. Is interaction only for categorical variables (Answer: No, but should we worry about it?  Are there likely candidates? Should we simplify?)
(a) How do we find a good interaction with just 1 categorical variable?
3. Do we want to error on the side of more predictors (less bias) or fewer predictors (less variance)?
4. Should we do cross validation?
5. Should we check AIC/BIC/Press statistic (43)
6. Test predictive ability?
7. Should we try to get better linear fits by dealing with outliers? (apparenty we don't have to check categorical predictors for outliers (see discussion notes))
8. (partial regression plot, pg. 48, module 8)


```{r}
jh_data$Pos<-factor(jh_data$Pos)
is.factor(jh_data$Pos)

levels(jh_data$Pos)
contrasts(jh_data$Pos)

# equality of variance assumption test.  H0 variances are the same, HA variances are not the same for all classes
library(lawstat)
levene.test(jh_data$Actual.t,jh_data$Pos)
# greater than 0.05 so we reject H0 and we are in good shape

a1<-subset(jh_data,Pos=="C")
a2<-subset(jh_data,Pos=="C-PF")
a3<-subset(jh_data,Pos=="PF")
a4<-subset(jh_data,Pos=="PF-C")
a5<-subset(jh_data,Pos=="PF-SF")
a6<-subset(jh_data,Pos=="PG")
a7<-subset(jh_data,Pos=="PG-SG")
a8<-subset(jh_data,Pos=="SF")
a9<-subset(jh_data,Pos=="SF-C")
a10<-subset(jh_data,Pos=="SF-PF")
a11<-subset(jh_data,Pos=="SF-SG")
a12<-subset(jh_data,Pos=="SG")
a13<-subset(jh_data,Pos=="SG-PG")
a14<-subset(jh_data,Pos=="SG-SF")


reg1<-lm(Actual.t~PTS,data=a1)
reg2<-lm(Actual.t~PTS,data=a2)
reg3<-lm(Actual.t~PTS,data=a3)
reg4<-lm(Actual.t~PTS,data=a4)
reg5<-lm(Actual.t~PTS,data=a5)
reg6<-lm(Actual.t~PTS,data=a6)
reg7<-lm(Actual.t~PTS,data=a7)
reg8<-lm(Actual.t~PTS,data=a8)
reg9<-lm(Actual.t~PTS,data=a9)
reg10<-lm(Actual.t~PTS,data=a10)
reg11<-lm(Actual.t~PTS,data=a11)
reg12<-lm(Actual.t~PTS,data=a12)
reg13<-lm(Actual.t~PTS,data=a13)
reg14<-lm(Actual.t~PTS,data=a14)

plot(jh_data$PTS,jh_data$Actual.t, main="Actual income against PTS, by Pos")
points(a2$PTS,a2$Actual.t, pch=2, col="red")
points(a3$PTS,a3$Actual.t, pch=3, col="blue")
points(a4$PTS,a4$Actual.t, pch=4, col="green")
points(a5$PTS,a5$Actual.t, pch=5, col="orange")
points(a6$PTS,a6$Actual.t, pch=6, col="purple")
points(a7$PTS,a7$Actual.t, pch=7, col="pink")
points(a8$PTS,a8$Actual.t, pch=8, col="seagreen2")
points(a9$PTS,a9$Actual.t, pch=9, col="salmon3")
points(a10$PTS,a10$Actual.t, pch=10, col="royalblue4")
points(a11$PTS,a11$Actual.t, pch=11, col="rosybrown4")
points(a12$PTS,a12$Actual.t, pch=12, col="skyblue2")
points(a13$PTS,a13$Actual.t, pch=13, col="slateblue2")
points(a14$PTS,a14$Actual.t, pch=14, col="slategray2")

# Note: I've commented out lines that were failing because they only have one or two values
abline(reg1,lty=1)
# abline(reg2,lty=2, col="red")
abline(reg3,lty=3, col="blue")
abline(reg4,lty=4, col="green")
abline(reg5,lty=5, col="orange")
abline(reg6,lty=6, col="purple")
# abline(reg7,lty=7, col="pink")
abline(reg8,lty=8, col="seagreen2")
# abline(reg9,lty=9, col="salmon3")
abline(reg10,lty=10, col="royalblue4")
# abline(reg11,lty=11, col="rosybrown4")
abline(reg12,lty=12, col="skyblue2")
abline(reg13,lty=13, col="slateblue2")
# abline(reg14,lty=14, col="slategray2")

legend("right", c("C","C-PF","PF","PF-C","PF-SF","PG","PG-SG","SF","SF-C","SF-PF","SF-SG","SG","SG-PG","SG-SF"), lty=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14), pch=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14), col=c("black","red","blue","green","orange","purple","pink","seagreen2","salmon3","royalblue4","rosybrown4","skyblue2","slateblue2","slategray2"))

# some of these lines are a little nuts (PF-SF, SG-PG), these should be collapsed into other fields

# run a boxplot to see if it makes sense to combine others of these
boxplot(jh_data$Actual.t~jh_data$Pos, main="Boxplot of Actual Pay by Position")

# colapse classes
# create new collapsed class
jh_data$Pos.colapsed <- jh_data$Pos

# Eliminate all the double position classes. When no clear preference in the box plot pick the primary position.

# C-PF doesn't really fit anywhere, put with C
jh_data$Pos.colapsed[which(jh_data$Pos == "C-PF")] = "C"
# PF-C closer to C than PF. 
jh_data$Pos.colapsed[which(jh_data$Pos == "PF-C")] = "C"
# PF-SF is unique, we will keep that one around
# PG-SG we can group with PG (no natural match)
jh_data$Pos.colapsed[which(jh_data$Pos == "PG-SG")] = "PG"
# SF-C we will group with SF, no natural match
jh_data$Pos.colapsed[which(jh_data$Pos == "SF-C")] = "SF"
# SF-PF seems like a more natural match with SF
jh_data$Pos.colapsed[which(jh_data$Pos == "SF-PF")] = "SF"
# SF-SG seems like a better match with SF
jh_data$Pos.colapsed[which(jh_data$Pos == "SF-SG")] = "SF"
# SG-PG we will go with SG, no obvious match
jh_data$Pos.colapsed[which(jh_data$Pos == "SG-PG")] = "SF"
# SG-SF we will go with SG, no obvious match
jh_data$Pos.colapsed[which(jh_data$Pos == "SG-SF")] = "SF"

boxplot(jh_data$Actual.t~jh_data$Pos.colapsed, main="Boxplot of Actual Pay by Colapsed Position")

# Check the new model
reduced.t2<-lm(jh_data$Actual.t ~ yrs_exp + num_teams_played + GS. +
    drafted  + G + Pos.colapsed + AST + draft_num + DRB + twitter_followers +
    FGA + PF + injury_yrs, data=jh_data)
summary(reduced.t2)

# equality of variance assumption test.  H0 variances are the same, HA variances are not the same for all classes
library(lawstat)
levene.test(jh_data$Actual.t,jh_data$Pos.colapsed)
# greater than 0.05 so we reject H0 and we are in good shape

# replot the graph, look for nonlinear relationships (not paralel lines) add in interaction and compare partial f tests
b1<-subset(jh_data,Pos.colapsed=="C")
b2<-subset(jh_data,Pos.colapsed=="PF")
b3<-subset(jh_data,Pos.colapsed=="PF-SF")
b4<-subset(jh_data,Pos.colapsed=="PG")
b5<-subset(jh_data,Pos.colapsed=="SF")
b6<-subset(jh_data,Pos.colapsed=="SG")


reg1<-lm(Actual.t~PTS,data=b1)
reg2<-lm(Actual.t~PTS,data=b2)
reg3<-lm(Actual.t~PTS,data=b3)
reg4<-lm(Actual.t~PTS,data=b4)
reg5<-lm(Actual.t~PTS,data=b5)
reg6<-lm(Actual.t~PTS,data=b6)

plot(jh_data$PTS,jh_data$Actual.t, main="Actual income against PTS, by Pos")
points(b2$PTS,b2$Actual.t, pch=2, col="red")
points(b3$PTS,b3$Actual.t, pch=3, col="blue")
points(b4$PTS,b4$Actual.t, pch=4, col="green")
points(b5$PTS,b5$Actual.t, pch=5, col="orange")
points(b6$PTS,b6$Actual.t, pch=6, col="purple")

abline(reg1,lty=1)
abline(reg2,lty=2, col="red")
abline(reg3,lty=3, col="blue")
abline(reg4,lty=4, col="green")
abline(reg5,lty=5, col="orange")
abline(reg6,lty=6, col="purple")

legend("right", c("C","PF","PF-SF","PG","SF","SG"), lty=c(1,2,3,4,5,6), pch=c(1,2,3,4,5,6), col=c("black","red","blue","green","orange","purple"))


# lines are not paralel indicating a posible interaction.  Let's see what that model looks like.
reduced.t3<-lm(jh_data$Actual.t ~ yrs_exp + num_teams_played + GS. +
    drafted  + G * Pos.colapsed + AST + draft_num + DRB + twitter_followers +
    FGA + PF + injury_yrs, data=jh_data)
summary(reduced.t3)

# partial f test, should we keep the interaction?
anova(reduced.t3,reduced.t2)
# fail to reject the null, we will remove the interaction
```

